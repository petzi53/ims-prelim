# Summarizing and visualizing data {#summarizing-visualizing-data}

::: {.chapterintro}
This chapter focuses on the mechanics and construction of summary statistics and graphs.
We use statistical software for generating the summaries and graphs presented in this chapter and book.
However, since this might be your first exposure to these concepts, we take our time in this chapter to detail how to create them.
Mastery of the content presented in this chapter will be crucial for understanding the methods and techniques introduced in rest of the book.
:::

## Exploring numerical data {#numerical-data}

In this section we will explore techniques for summarizing numerical variables.
For example, consider the `loan_amount` variable from the `loan50` data set, which represents the loan size for each of 50 loans in the data set.
This variable is numerical since we can sensibly discuss the numerical difference of the size of two loans.
On the other hand, area codes and zip codes are not numerical, but rather they are categorical variables.

Throughout this section and the next, we will apply numerical methods using the `loan50` and `county` data sets, which were introduced in Section \@ref(data-basics).
If you'd like to review the variables from either data set, see Tables \@ref(tab:loan-50-variables) and \@ref(tab:county-variables).

::: {.data}
The [`county`](http://openintrostat.github.io/usdata/reference/county.html) data can be found in the [usdata](http://openintrostat.github.io/usdata) package and the [`loan50`](http://openintrostat.github.io/openintro/reference/loan50.html) data can be found in the [openintro](http://openintrostat.github.io/openintro) package.
:::

### Scatterplots for paired data {#scatterplots}

\index{data!loan50|(}

A **scatterplot** provides a case-by-case view of data for two numerical variables.
In Figure \@ref(fig:county-multi-unit-homeownership), a scatterplot was used to examine the homeownership rate against the fraction of housing units that were part of multi-unit properties (e.g., apartments) in the `county` data set.
Another scatterplot is shown in Figure \@ref(fig:loan50-amount-income), comparing the total income of a borrower `total_income` and the amount they borrowed `loan_amount` for the `loan50` data set.
In any scatterplot, each point represents a single case.
Since there are `r nrow(loan50)` cases in `loan50`, there are `r nrow(loan50)` points in Figure \@ref(fig:loan50-amount-income).

```{r include=FALSE}
terms_chp_2 <- c("scatterplot")
```

```{r loan50-amount-income, fig.cap = "A scatterplot of `loan_amount` versus `total_income` for the `loan50` data set.", warning=FALSE}
ggplot(loan50, aes(x = total_income, y = loan_amount)) +
  geom_point(alpha = 0.6, color = COL["blue", "full"], 
             fill = COL["blue", "full"], shape = 21, size = 3) +
  labs(x = "Total income", y = "Loan amount") +
  scale_x_continuous(labels = dollar_format(scale = 0.001, suffix = "K")) +
  scale_y_continuous(labels = dollar_format(scale = 0.001, suffix = "K"))
```

Looking at Figure \@ref(fig:loan50-amount-income), we see that there are many borrowers with income below \$100,000 on the left side of the graph, while there are a handful of borrowers with income above \$250,000.

```{r median-hh-income-poverty, fig.cap = "A scatterplot of the median household income against the poverty rate for the `county` dataset. Data are from 2017. A statistical model has also been fit to the data and is shown as a dashed line.", warning=FALSE, message=FALSE}
ggplot(county, aes(x = poverty/100, y = median_hh_income)) +
  geom_point(alpha = 0.3, color = COL["blue", "full"], 
             fill = COL["black", "full"], shape = 21, size = 3) +
  geom_smooth(linetype = "dashed", color = "gray", se = FALSE) +
  labs(x = "Poverty rate",y = "Median household income") +
  scale_x_continuous(labels = percent_format(accuracy = 1)) +
  scale_y_continuous(labels = dollar_format(scale = 0.001, suffix = "K"))
```

::: {.workedexample}
Figure \@ref(fig:median-hh-income-poverty) shows a plot of median household income against the poverty rate for `r nrow(county)` counties.
What can be said about the relationship between these variables?

------------------------------------------------------------------------

The relationship is evidently **nonlinear**, as highlighted by the dashed line.
This is different from previous scatterplots we have seen, which indicate very little, if any, curvature in the trend.
:::

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "nonlinear")
```

::: {.guidedpractice}
What do scatterplots reveal about the data, and how are they useful?[^summarizing-visualizing-data-1]
:::

[^summarizing-visualizing-data-1]: Answers may vary.
    Scatterplots are helpful in quickly spotting associations relating variables, whether those associations come in the form of simple trends or whether those relationships are more complex.

::: {.guidedpractice}
Describe two variables that would have a horseshoe-shaped association in a scatterplot $(\cap$ or $\frown).$[^summarizing-visualizing-data-2]
:::

[^summarizing-visualizing-data-2]: Consider the case where your vertical axis represents something "good" and your horizontal axis represents something that is only good in moderation.
    Health and water consumption fit this description: we require some water to survive, but consume too much and it become toxic and can kill a person.

### Dot plots and the mean {#dotplots}

Sometimes we are interested in the distribution of a single variable.
In these cases, a dot plot provides the most basic of displays.
A **dot plot** is a one-variable scatterplot; an example using the interest rate of `r nrow(loan50)` loans is shown in Figure \@ref(fig:loan-int-rate-dotplot).

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "dot plot")
```

```{r loan-int-rate-dotplot, fig.cap="A dot plot of `interest_rate` for the `loan50` dataset. The rates have been rounded to the nearest percent in this plot, and the distribution's mean is shown as a red triangle."}
loan50_interest_rate_mean <- mean(loan50$interest_rate)

ggplot(loan50, aes(x = interest_rate)) +
  geom_dotplot(fill = COL["blue", "full"], color = COL["blue", "full"]) +
  labs(x = "Interest rate") +
  scale_x_continuous(labels = label_percent(scale = 1)) +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank()) +
  geom_polygon(
    data = data.frame(x = c(loan50_interest_rate_mean - 1, loan50_interest_rate_mean + 1, loan50_interest_rate_mean), 
                      y = c(-0.1, -0.1, 0)),
    aes(x = x, y = y),
    fill = COL["red", "full"]
  )
```

The **mean**, often called the **average** is a common way to measure the center of a **distribution** of data.
To compute the mean interest rate, we add up all the interest rates and divide by the number of observations.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "mean", "average", "distribution")
```

The sample mean is often labelled $\bar{x}.$ The letter $x$ is being used as a generic placeholder for the variable of interest and the bar over the $x$ communicates we're looking at the average interest rate, which for these 50 loans is `r round(loan50_interest_rate_mean, 2)`%.
It's useful to think of the mean as the balancing point of the distribution, and it's shown as a triangle in Figure \@ref(fig:loan-int-rate-dotplot).

::: {.important}
**Mean.** The sample mean can be calculated as the sum of the observed values divided by the number of observations:

$$ \bar{x} = \frac{x_1 + x_2 + \cdots + x_n}{n} $$
:::

::: {.guidedpractice}
Examine the equation for the mean.
What does $x_1$ correspond to?
And $x_2$?
Can you infer a general meaning to what $x_i$ might represent?[^summarizing-visualizing-data-3]
:::

[^summarizing-visualizing-data-3]: $x_1$ corresponds to the interest rate for the first loan in the sample, $x_2$ to the second loan's interest rate, and $x_i$ corresponds to the interest rate for the $i^{th}$ loan in the data set.
    For example, if $i = 4,$ then we're examining $x_4,$ which refers to the fourth observation in the data set.

::: {.guidedpractice}
What was $n$ in this sample of loans?[^summarizing-visualizing-data-4]
:::

[^summarizing-visualizing-data-4]: The sample size was $n = 50.$

The `loan50` data set represents a sample from a larger population of loans made through Lending Club.
We could compute a mean for the entire population in the same way as the sample mean.
However, the population mean has a special label: $\mu.$ The symbol $\mu$ is the Greek letter *mu* and represents the average of all observations in the population.
Sometimes a subscript, such as $_x,$ is used to represent which variable the population mean refers to, e.g., $\mu_x.$ Often times it is too expensive to measure the population mean precisely, so we often estimate $\mu$ using the sample mean, $\bar{x}.$

::: {.pronunciation}
The Greek letter $\mu$ is pronounced *mu*, listen to the pronunciation [here](https://youtu.be/PStgY5AcEIw?t=47).
:::

::: {.workedexample}
Although we don't have an ability to *calculate* the average interest rate across all loans in the populations, we can *estimate* the population value using the sample data.
Based on the sample of 50 loans, what would be a reasonable estimate of $\mu_x,$ the mean interest rate for all loans in the full data set?

------------------------------------------------------------------------

The sample mean, `r round(loan50_interest_rate_mean, 2)`, provides a rough estimate of $\mu_x.$ While it is not perfect, this is our single best guess **point estimate**\index{point estimate} of the average interest rate of all the loans in the population under study.
In Chapter \@ref(intro-stat-inference) and beyond, we will develop tools to characterize the accuracy of point estimates, like the sample mean.
As you might have guessed, point estimates based on larger samples tend to be more accurate than those based on smaller samples.
:::

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "point estimate")
```

The mean is useful because it allows us to rescale or standardize a metric into something more easily interpretable and comparable.
Suppose we would like to understand if a new drug is more effective at treating asthma attacks than the standard drug.
A trial of 1500 adults is set up, where 500 receive the new drug, and 1000 receive a standard drug in the control group:

```{r}
drug_asthma <- tribble(
  ~x, ~`New drug`, ~`Standard drug`,
  "Number of patients", 500, 1000,
  "Total asthma attacks", 200, 300
)
drug_asthma %>%
  kable(
    col.names = c("", "New drug", "Standard drug"),
    align = c("lcc")
  )
```

Comparing the raw counts of 200 to 300 asthma attacks would make it appear that the new drug is better, but this is an artifact of the imbalanced group sizes.
Instead, we should look at the average number of asthma attacks per patient in each group:

-   New drug: $200 / 500 = 0.4$ asthma attacks per patient
-   Standard drug: $300 / 1000 = 0.3$ asthma attacks per patient

The standard drug has a lower average number of asthma attacks per patient than the average in the treatment group.

::: {.workedexample}
Provide another examples where the mean is useful for making comparisons.

------------------------------------------------------------------------

Emilio opened a food truck last year where he sells burritos, and his business has stabilized over the last 3 months.
Over that 3 month period, he has made \$11,000 while working 625 hours.
Emilio's average hourly earnings provides a useful statistic for evaluating whether his venture is, at least from a financial perspective, worth it:

$$ \frac{\$11000}{625\text{ hours}} = \$17.60\text{ per hour} $$

By knowing his average hourly wage, Emilio now has put his earnings into a standard unit that is easier to compare with many other jobs that he might consider.
:::

::: {.workedexample}
Suppose we want to compute the average income per person in the US.
To do so, we might first think to take the mean of the per capita incomes across the 3,142 counties in the `county` data set.
What would be a better approach?

------------------------------------------------------------------------

The `county` data set is special in that each county actually represents many individual people.
If we were to simply average across the `income` variable, we would be treating counties with 5,000 and 5,000,000 residents equally in the calculations.
Instead, we should compute the total income for each county, add up all the counties' totals, and then divide by the number of people in all the counties.
If we completed these steps with the `county` data, we would find that the per capita income for the US is \$30,861.
Had we computed the *simple* mean of per capita income across counties, the result would have been just \$26,093!

This example used what is called a **weighted mean**.
For more information on this topic, check out the following online supplement regarding [weighted means](https://www.openintro.org/go/?id=stat_extra_weighted_mean).
:::

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "weighted mean")
```

### Histograms and shape {#histograms}

Dot plots show the exact value for each observation.
They are useful for small data sets but can become hard to read with larger samples.
Rather than showing the value of each observation, we prefer to think of the value as belonging to a *bin*.
For example, in the `loan50` data set, we created a table of counts for the number of loans with interest rates between 5.0% and 7.5%, then the number of loans with rates between 7.5% and 10.0%, and so on.
Observations that fall on the boundary of a bin (e.g., 10.00%) are allocated to the lower bin.
The tabulation is shown in Table \@ref(tab:binnedIntRateAmountTable), and the binned counts are plotted as bars in Figure \@ref(fig:loan50IntRateHist) into what is called a **histogram**.
Note that the histogram resembles a more heavily binned version of the stacked dot plot shown in Figure \@ref(fig:loan-int-rate-dotplot).

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "histogram")
```

```{r binnedIntRateAmountTable}
loan50 %>%
  mutate(interest_rate_cat = cut(interest_rate, breaks = seq(5, 27.5, 2.5))) %>%
  count(interest_rate_cat, name = "Count") %>%
  separate(interest_rate_cat, into = c("lower", "upper"), sep = ",") %>%
  mutate(
    lower = str_remove(lower, "\\("),
    upper = str_remove(upper, "]"),
    lower = paste0(lower, "%"),
    upper = paste0(upper, "%")
  ) %>%
  unite("Interest rate", lower:upper, sep = " - ") %>%
  pivot_wider(names_from = `Interest rate`, values_from = Count) %>%
  bind_cols(tibble("Interest rate" = "Count"), .) %>%
  kable(align = "lccccccccc", caption = "Counts for the binned `interest_rate` data.")
```

```{r loan50IntRateHist, fig.cap = "A histogram of `interest_rate`. This distribution is strongly skewed to the right."}
ggplot(loan50, aes(x = interest_rate)) +
  geom_histogram(breaks = seq(5, 27.5, 2.5), fill = COL["blue", "full"], color = "#4287aa") +
  labs(x = "Interest rate", y = "Frequency") +
  scale_x_continuous(breaks = seq(5, 25, 5), labels = label_percent(scale = 1, accuracy = 1))
```

Histograms provide a view of the **data density**.
Higher bars represent where the data are relatively more common.
For instance, there are many more loans with rates between 5% and 10% than loans with rates between 20% and 25% in the data set.
The bars make it easy to see how the density of the data changes relative to the interest rate.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "data density")
```

Histograms are especially convenient for understanding the shape of the data distribution.
Figure \@ref(fig:loan50IntRateHist) suggests that most loans have rates under 15%, while only a handful of loans have rates above 20%.
When data trail off to the right in this way and has a longer right **tail**, the shape is said to be **right skewed**.[^summarizing-visualizing-data-5]

[^summarizing-visualizing-data-5]: Other ways to describe data that are right skewed: skewed to the right, skewed to the high end, or skewed to the positive end.

Data sets with the reverse characteristic -- a long, thinner tail to the left -- are said to be **left skewed**.
We also say that such a distribution has a long left tail.
Data sets that show roughly equal trailing off in both directions are called **symmetric**.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "tail", "right skewed", "left skewed", "symmetric")
```

::: {.important}
When data trail off in one direction, the distribution has a **long tail**.
If a distribution has a long left tail, it is left skewed.
If a distribution has a long right tail, it is right skewed.
:::

::: {.guidedpractice}
Besides the mean (since it was labelled), what can you see in the dot plot in Figure \@ref(fig:loan-int-rate-dotplot) that you cannot see in the histogram in Figure \@ref(fig:loan50IntRateHist)?[^summarizing-visualizing-data-6]
:::

[^summarizing-visualizing-data-6]: The interest rates for individual loans.

In addition to looking at whether a distribution is skewed or symmetric, histograms can be used to identify modes.
A **mode** is represented by a prominent peak in the distribution.
There is only one prominent peak in the histogram of `interest_rate`.

A definition of *mode* sometimes taught in math classes is the value with the most occurrences in the data set.
However, for many real-world data sets, it is common to have *no* observations with the same value in a data set, making this definition impractical in data analysis.

Figure \@ref(fig:singleBiMultiModalPlots) shows histograms that have one, two, or three prominent peaks.
Such distributions are called **unimodal**, **bimodal**, and **multimodal**, respectively.
Any distribution with more than 2 prominent peaks is called multimodal.
Notice that there was one prominent peak in the unimodal distribution with a second less prominent peak that was not counted since it only differs from its neighboring bins by a few observations.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "unimodal", "bimodal", "multimodal")
```

```{r singleBiMultiModalPlots, fig.cap = "Counting only prominent peaks, the distributions are (left to right) unimodal, bimodal, and multimodal. Note that the left plot is unimodal because we are counting prominent peaks, not just any peak.", fig.asp=0.33, out.width = "90%", warning=FALSE}
df_modes <- tibble(
  uni   = rchisq(65, 6),
  bi    = c(rchisq(25, 5.8), rnorm(40, 20, 2)),
  multi =  c(rchisq(25, 3), rnorm(25, 15), rnorm(15, 25, 1.5))
)

p_uni   <- ggplot(df_modes, aes(x = uni)) + 
  geom_histogram(binwidth = 2, fill = COL["blue", "full"], color = "#4287aa") +
  labs(x = NULL, y = NULL) +
  ylim(0, 23) +
  xlim(0, 30)
p_bi    <- ggplot(df_modes, aes(x = bi)) + 
  geom_histogram(binwidth = 2, fill = COL["blue", "full"], color = "#4287aa") +
  labs(x = NULL, y = NULL) +
  ylim(0, 23) +
  xlim(0, 30)
p_multi <- ggplot(df_modes, aes(x = multi)) + 
  geom_histogram(binwidth = 2, fill = COL["blue", "full"], color = "#4287aa") +
  labs(x = NULL, y = NULL) +
  ylim(0, 23) +
  xlim(0, 30)

p_uni + p_bi + p_multi
```

::: {.workedexample}
Figure \@ref(fig:loan50IntRateHist) reveals only one prominent mode in the interest rate.
Is the distribution unimodal, bimodal, or multimodal?[^summarizing-visualizing-data-7]
:::

[^summarizing-visualizing-data-7]: Unimodal Remember that *uni* stands for 1 (think *uni*cycles).
    Similarly, *bi* stands for 2 (think *bi*cycles).
    We are hoping a *multi*cycle will be invented to complete this analogy.

::: {.guidedpractice}
Height measurements of young students and adult teachers at a K-3 elementary school were taken.
How many modes would you expect in this height data set?[^summarizing-visualizing-data-8].
:::

[^summarizing-visualizing-data-8]: There might be two height groups visible in the data set: one of the students and one of the adults.
    That is, the data are probably bimodal.

Looking for modes isn't about finding a clear and correct answer about the number of modes in a distribution, which is why *prominent*\index{prominent} is not rigorously defined in this book.
The most important part of this examination is to better understand your data.

### Variance and standard deviation {#variance-sd}

The mean was introduced as a method to describe the center of a data set, and **variability**\index{variability} in the data is also important.
Here, we introduce two measures of variability: the variance and the standard deviation.
Both of these are very useful in data analysis, even though their formulas are a bit tedious to calculate by hand.
The standard deviation is the easier of the two to comprehend, and it roughly describes how far away the typical observation is from the mean.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "variability")
```

We call the distance of an observation from its mean its **deviation**.
Below are the deviations for the $1^{st},$ $2^{nd},$ $3^{rd},$ and $50^{th}$ observations in the `interest_rate` variable:

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "deviation")
```

```{r include=FALSE}
loan50_interest_rate_deviation <- round(loan50$interest_rate - loan50_interest_rate_mean, 2)
loan50_interest_rate_deviation_squared <- round(loan50_interest_rate_deviation^2, 2)
loan50_interest_rate_var <- var(loan50$interest_rate)
loan50_interest_rate_sd <- sd(loan50$interest_rate)
```

$$ x_1 - \bar{x} = `r round(loan50$interest_rate[1], 2)` - `r round(loan50_interest_rate_mean, 2)` = `r loan50_interest_rate_deviation[1]` $$ $$ x_2 - \bar{x} = `r round(loan50$interest_rate[2], 2)` - `r round(loan50_interest_rate_mean, 2)` = `r loan50_interest_rate_deviation[2]` $$ $$ x_3 - \bar{x} = `r round(loan50$interest_rate[3], 2)` - `r round(loan50_interest_rate_mean, 2)` = `r loan50_interest_rate_deviation[3]` $$ $$ \vdots $$ $$ x_{50} - \bar{x} = `r round(loan50$interest_rate[50], 2)` - `r round(loan50_interest_rate_mean, 2)` = `r loan50_interest_rate_deviation[50]` $$

If we square these deviations and then take an average, the result is equal to the sample **variance**, denoted by $s^2$:

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "variance")
```

```{=tex}
\begin{align}
s^2 &= \frac{(`r loan50_interest_rate_deviation[1]`)^2 + (`r loan50_interest_rate_deviation[2]`)^2 + (`r loan50_interest_rate_deviation[3]`)^2 + \cdots + (`r loan50_interest_rate_deviation[50]`)^2}{50 - 1} \\
&= \frac{`r loan50_interest_rate_deviation_squared[1]` + `r loan50_interest_rate_deviation_squared[2]` + \cdots + `r loan50_interest_rate_deviation_squared[50]`}{49} = `r round(loan50_interest_rate_var, 2)`
\end{align}
```
We divide by $n - 1,$ rather than dividing by $n,$ when computing a sample's variance; there's some mathematical nuance here, but the end result is that doing this makes this statistic slightly more reliable and useful.

Notice that squaring the deviations does two things.
First, it makes large values relatively much larger.
Second, it gets rid of any negative signs.

The **standard deviation** is defined as the square root of the variance:

$$ s = \sqrt{`r round(loan50_interest_rate_var, 2)`} = `r round(loan50_interest_rate_sd, 2)` $$

While often omitted, a subscript of $_x$ may be added to the variance and standard deviation, i.e., $s_x^2$ and $s_x^{},$ if it is useful as a reminder that these are the variance and standard deviation of the observations represented by $x_1,$ $x_2,$ ..., $x_n.$

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "standard deviation")
```

::: {.important}
**Variance and standard deviation.** The variance is the average squared distance from the mean.
The standard deviation is the square root of the variance.
The standard deviation is useful when considering how far the data are distributed from the mean.

The standard deviation represents the typical deviation of observations from the mean.
Often about 68% of the data will be within one standard deviation of the mean and about 95% will be within two standard deviations.
However, these percentages are not strict rules.
:::

Like the mean, the population values for variance and standard deviation have special symbols: $\sigma^2$ for the variance and $\sigma$ for the standard deviation.

::: {.pronunciation}
The Greek letter $\sigma$ is pronounced *sigma*, listen to the pronunciation [here](https://youtu.be/PStgY5AcEIw?t=72).
:::

```{r sdRuleForIntRate, fig.cap = "For the `interest_rate` variable, 34 of the 50 loans (68%) had interest rates within 1 standard deviation of the mean, and 48 of the 50 loans (96%) had rates within 2 standard deviations. Usually about 68% of the data are within 1 standard deviation of the mean and 95% within 2 standard deviations, though this is far from a hard rule."}
box_sd1 <- data.frame(
  x = c(loan50_interest_rate_mean - loan50_interest_rate_sd,
        loan50_interest_rate_mean - loan50_interest_rate_sd,
        loan50_interest_rate_mean + loan50_interest_rate_sd,
        loan50_interest_rate_mean + loan50_interest_rate_sd),
  y = c(0, 17, 17, 0)
)

box_sd2 <- data.frame(
  x = c(loan50_interest_rate_mean - 2*loan50_interest_rate_sd,
        loan50_interest_rate_mean - 2*loan50_interest_rate_sd,
        loan50_interest_rate_mean + 2*loan50_interest_rate_sd,
        loan50_interest_rate_mean + 2*loan50_interest_rate_sd),
  y = c(0, 17, 17, 0)
)

box_sd3 <- data.frame(
  x = c(loan50_interest_rate_mean - 3*loan50_interest_rate_sd,
        loan50_interest_rate_mean - 3*loan50_interest_rate_sd,
        loan50_interest_rate_mean + 3*loan50_interest_rate_sd,
        loan50_interest_rate_mean + 3*loan50_interest_rate_sd),
  y = c(0, 17, 17, 0)
)

ggplot(loan50, aes(x = interest_rate)) +
  labs(x = "Interest rate", y = "Frequency") +
  geom_histogram(breaks = seq(5, 27.5, 2.5), fill = COL["blue", "full"], color = "#4287aa") +
  scale_x_continuous(breaks = seq(-5, 25, 5), labels = label_percent(scale = 1, accuracy = 1)) +
  geom_polygon(data = box_sd1, aes(x = x, y = y), fill = COL["gray", "full"], alpha = 0.3) +
  geom_polygon(data = box_sd2, aes(x = x, y = y), fill = COL["gray", "full"], alpha = 0.3) +
  geom_polygon(data = box_sd3, aes(x = x, y = y), fill = COL["gray", "full"], alpha = 0.3)
```

```{r severalDiffDistWithSdOf1, fig.cap = "Three very different population distributions with the same mean (0) and standard deviation (1)."}
x1 <- c(rep(-0.975, 1000), rep(0.975, 1000))
x1 <- (x1-mean(x1))/sd(x1)
x2 <- rnorm(2000)
x2 <- (x2-mean(x2))/sd(x2)
x3 <- qchisq(seq(0.26, 0.8, 0.0005), 4)
x3 <- (x3-mean(x3))/sd(x3)

dists_mean0_sd1 <- tibble(
  x = c(x1, x2, x3),
  group = c(rep("A", length(x1)), rep("B", length(x2)), rep("C", length(x3)))
)

ggplot(dists_mean0_sd1, aes(x = x)) +
  geom_histogram(binwidth = 1, fill = COL["blue", "full"], color = "#4287aa") +
  facet_grid(group ~ ., scales = "free_y") +
  theme(# remove y axis
        axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank(),
        # strip facet labels
        strip.background = element_blank(),
        strip.text.y = element_blank()) +
  scale_x_continuous(breaks = seq(-3, 3, 1)) +
  labs(x = NULL) +
  geom_polygon(data = data.frame(x = c(-1, -1, 1, 1), y = c(0, 1000, 1000, 0)), 
               aes(x = x, y = y), fill = COL["gray", "full"], alpha = 0.3) +
  geom_polygon(data = data.frame(x = c(-2, -2, 2, 2), y = c(0, 1000, 1000, 0)), 
               aes(x = x, y = y), fill = COL["gray", "full"], alpha = 0.3) +
  geom_polygon(data = data.frame(x = c(-3, -3, 3, 3), y = c(0, 1000, 1000, 0)), 
               aes(x = x, y = y), fill = COL["gray", "full"], alpha = 0.3)
```

::: {.guidedpractice}
A good description of the shape of a distribution should include modality and whether the distribution is symmetric or skewed to one side.
Using Figure \@ref(fig:severalDiffDistWithSdOf1) as an example, explain why such a description is important.[^summarizing-visualizing-data-9]
:::

[^summarizing-visualizing-data-9]: Figure \@ref(fig:severalDiffDistWithSdOf1) shows three distributions that look quite different, but all have the same mean, variance, and standard deviation.
    Using modality, we can distinguish between the first plot (bimodal) and the last two (unimodal).
    Using skewness, we can distinguish between the last plot (right skewed) and the first two.
    While a picture, like a histogram, tells a more complete story, we can use modality and shape (symmetry/skew) to characterize basic information about a distribution.

::: {.workedexample}
Describe the distribution of the `interest_rate` variable using the histogram in Figure \@ref(fig:loan50IntRateHist).
The description should incorporate the center, variability, and shape of the distribution, and it should also be placed in context.
Also note any especially unusual cases.

------------------------------------------------------------------------

The distribution of interest rates is unimodal and skewed to the high end.
Many of the rates fall near the mean at 11.57%, and most fall within one standard deviation (5.05%) of the mean.
There are a few exceptionally large interest rates in the sample that are above 20%.
:::

In practice, the variance and standard deviation are sometimes used as a means to an end, where the "end" is being able to accurately estimate the uncertainty associated with a sample statistic.
For example, in Chapter \@ref(intro-stat-inference) the standard deviation is used in calculations that help us understand how much a sample mean varies from one sample to the next.

### Box plots, quartiles, and the median

A **box plot** summarizes a data set using five statistics while also identifying unusual observations.
Figure \@ref(fig:loan-int-rate-boxplot-dotplot) provides a dot plot alongside a box plot of the `interest_rate` variable from the `loan50` data set.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "box plot")
```

```{r loan-int-rate-boxplot-dotplot, fig.cap="Plot A shows a dot plot and Plot B shows a box plot of the distribution of interest rates from the `loan50` dataset."}
p_dotplot <- ggplot(loan50, aes(x = interest_rate)) + 
  geom_dotplot(binwidth = 1, dotsize = 0.7, fill = COL["blue", "full"], color = COL["blue", "full"]) +
  labs(x = NULL, y = NULL) +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank()) +
  scale_x_continuous(labels = percent_format(scale = 1, accuracy = 1),
                     limits = c(0, 30))

p_boxplot <- ggplot(loan50, aes(x = interest_rate)) + 
  geom_boxplot(outlier.size = 2.5, outlier.color = COL["blue", "full"]) +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank()) +
  labs(x = "Interest rate") +
  scale_x_continuous(labels = percent_format(scale = 1, accuracy = 1),
                     limits = c(0, 30))

p_dotplot / 
  p_boxplot +
  plot_annotation(tag_levels = "A") & 
  theme(plot.tag = element_text(size = 10))
```

The dark line inside the box represents the **median**, which splits the data in half.
50% of the data fall below this value and 50% fall above it.
Since in the `loan50` dataset there are `r nrow(loan50)` observations (an even number), the median is defined as the average of the two observations closest to the $50^{th}$ percentile.
Table \@ref(tab:loan50-int-rate-sorted) shows all interest rates, arranged in ascending order.
We can see that the $25^{th}$ and the $26^{th}$ values are both `r sort(loan50$interest_rate)[25]`, which corresponds to the dark line in the box plot in Figure \@ref(fig:loan-int-rate-boxplot-dotplot).

```{r loan50-int-rate-sorted, comment=""}
loan50_interest_rate_sorted <- sort(loan50$interest_rate)
matrix(
  c(
    loan50_interest_rate_sorted[1:10],
    loan50_interest_rate_sorted[11:20],
    loan50_interest_rate_sorted[21:30],
    loan50_interest_rate_sorted[31:40],
    loan50_interest_rate_sorted[41:50]
  ),
  byrow = TRUE,
  ncol = 10
) %>%
  data.frame(row.names = c("1", "10", "20", "30", "40")) %>%
  kable(
    col.names = as.character(1:10),
    caption = "Interest rates from the `loan50` dataset, arranged in ascending order."
  )
```

When there are an odd number of observations, there will be exactly one observation that splits the data into two halves, and in such a case that observation is the median (no average needed).

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "median")
```

::: {.important}
**Median: the number in the middle**\
If the data are ordered from smallest to largest, the **median** is the observation right in the middle.
If there are an even number of observations, there will be two values in the middle, and the median is taken as their average.
:::

The second step in building a box plot is drawing a rectangle to represent the middle 50% of the data.
The length of the the box is called the **interquartile range**, or **IQR** for short.
It, like the standard deviation, is a measure of \index{variability}variability in data.
The more variable the data, the larger the standard deviation and IQR tend to be.
The two boundaries of the box are called the **first quartile** (the $25^{th}$ percentile, i.e., 25% of the data fall below this value) and the **third quartile** (the $75^{th}$ percentile, i.e., 75% of the data fall below this value) \index{quartile!first quartile} \index{quartile!third quartile}, and these are often labelled $Q_1$ and $Q_3,$ respectively.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "interquartile range", "IQR", "first quartile", "third quartile", "percentile")
```

::: {.important}
**Interquartile range (IQR)** The IQR interquartile range is the length of the box in a box plot.
It is computed as $IQR = Q_3 - Q_1,$ where $Q_1$ and $Q_3$ are the $25^{th}$ and $75^{th}$ percentiles, respectively.

A $\alpha$ **percentile** is a number with $\alpha$% of the observations below and $100-\alpha$% of the observations above.
For example, the $90^{th}$ percentile of SAT scores is the value of the SAT score with 90% of students below that value and 10% of students above that value.
:::

::: {.guidedpractice}
What percent of the data fall between $Q_1$ and the median?
What percent is between the median and $Q_3$?[^summarizing-visualizing-data-10]
:::

[^summarizing-visualizing-data-10]: Since $Q_1$ and $Q_3$ capture the middle 50% of the data and the median splits the data in the middle, 25% of the data fall between $Q_1$ and the median, and another 25% falls between the median and $Q_3.$

Extending out from the box, the **whiskers** attempt to capture the data outside of the box.
The whiskers of a box plot reach to the minimum and the maximum values in the data, unless there are points that are considered unusually high or unusually low, which are identified as potential **outliers** by the box plot.
These are labelled with a dot on the box plot.
The purpose of labelling the outlying points -- instead of extending the whiskers to the minimum and maximum observed values -- is to help identify any observations that appear to be unusually distant from the rest of the data.
There are a variety of formulas for determining whether a particular data point is considered an outlier, and different statistical software use different formulas.
A commonly used formula is that any observation beyond $1.5\times IQR$ away from the first or the third quartile is considered an outlier.
In a sense, the box is like the body of the box plot and the whiskers are like its arms trying to reach the rest of the data, up to the outliers.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "outlier", "whiskers")
```

::: {.important}
**Outliers are extreme.** An **outlier** is an observation that appears extreme relative to the rest of the data.
Examining data for outliers serves many useful purposes, including

-   identifying strong skew \index{skew!strong} in the distribution,
-   identifying possible data collection or data entry errors, and
-   providing insight into interesting properties of the data.

Keep in mind, however, that some datasets have a naturally long skew and outlying points do **not** represent any sort of problem in the dataset.
:::

::: {.guidedpractice}
Using the box plot in Figure \@ref(fig:loan-int-rate-boxplot-dotplot), estimate the values of the $Q_1,$ $Q_3,$ and IQR for `interest_rate` in the `loan50` data set.[^summarizing-visualizing-data-11]
:::

[^summarizing-visualizing-data-11]: These visual estimates will vary a little from one person to the next: $Q_1 \approx$ 8%, $Q_3 \approx$ 14%, IQR $\approx$ 14 - 8 = 6%.

### Robust statistics

How are the **sample statistics** \index{sample statistic} of the `interest_rate` data set affected by the observation, 26.3%?
What would have happened if this loan had instead been only 15%?
What would happen to these summary statistics \index{summary statistic} if the observation at 26.3% had been even larger, say 35%?
The three conjectured scenarios are plotted alongside the original data in Figure \@ref(fig:loan-int-rate-robust-ex), and sample statistics are computed under each scenario in Table \@ref(tab:robustOrNotTable).

```{r loan-int-rate-robust-ex, fig.cap="Dot plots of the original interest rate data and two modified data sets.", fig.width = 8, fig.asp = 1.2}
loan50_original <- loan50 %>% 
  select(interest_rate) %>% 
  mutate(
    mark = if_else(interest_rate == 26.30, TRUE, FALSE)
    )

loan50_15 <- loan50 %>% 
  select(interest_rate) %>% 
  mutate(
    mark = if_else(interest_rate == 26.30, TRUE, FALSE),
    interest_rate = if_else(interest_rate == 26.30, 15, interest_rate)
    )

loan50_35 <- loan50 %>% 
  select(interest_rate) %>% 
  mutate(
    mark = if_else(interest_rate == 26.30, TRUE, FALSE),
    interest_rate = if_else(interest_rate == 26.30, 35, interest_rate)
    )     

p_original <- ggplot(loan50_original, aes(x = interest_rate)) +
  geom_dotplot(binwidth=1, dotsize = 0.7, fill = COL["blue", "full"], color = COL["blue", "full"]) +
  scale_x_continuous(
    labels = label_percent(scale = 1, accuracy = 1),
    limits = c(0, 35)
    ) +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank()) +
  gghighlight(mark) +
  labs(
    x = NULL,
    title = "Original data"
    )

p_15 <- ggplot(loan50_15, aes(x = interest_rate)) +
  geom_dotplot(binwidth=1, dotsize = 0.7, fill = COL["blue", "full"], color = COL["blue", "full"]) +
  scale_x_continuous(
    labels = label_percent(scale = 1, accuracy = 1),
    limits = c(0, 35)
    ) +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank()) +
  gghighlight(mark) +
  labs(
    x = NULL,
    title = "Move 26.3% to 15%"
    )

p_35 <- ggplot(loan50_35, aes(x = interest_rate)) +
  geom_dotplot(binwidth=1, dotsize = 0.7, fill = COL["blue", "full"], color = COL["blue", "full"]) +
  labs(x = "Interest rate") +
  scale_x_continuous(
    labels = label_percent(scale = 1, accuracy = 1),
    limits = c(0, 35)
    ) +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank()) +
  gghighlight(mark) +
  labs(
    x = NULL,
    title = "Move 26.3% to 35%"
    )

p_original / 
  p_15 /
  p_35
```

```{r robustOrNotTable}
loan50_robust_check <- bind_rows(
  loan50_original %>% mutate(Scenario = "Original data"),
  loan50_15 %>% mutate(Scenario = "Move 26.3% to 15%"),
  loan50_35 %>% mutate(Scenario = "Move 26.3% to 35%")
) %>%
  mutate(Scenario = fct_relevel(Scenario, "Original data", "Move 26.3% to 15%", "Move 26.3% to 35%"))

loan50_robust_check %>%
  group_by(Scenario) %>%
  summarise(
    Median = median(interest_rate),
    IQR = IQR(interest_rate),
    Mean = mean(interest_rate), 
    SD = sd(interest_rate)
    ) %>%
  kable(align = "lcccc", caption = "A comparison of how the median, IQR, mean, and standard deviation change as the value of an extereme observation from the original interest data changes.") %>%
  kable_styling("striped") %>%
  add_header_above(c(" " = 1, "Robust" = 2, "Not robust" = 2))
```

::: {.guidedpractice}
Which is more affected by extreme observations, the mean or median?
Is the standard deviation or IQR more affected by extreme observations?[^summarizing-visualizing-data-12]
:::

[^summarizing-visualizing-data-12]: Mean is affected more than the median.
    Standard deviation is affected more than the IQR.

The median and IQR are called **robust statistics** because extreme observations have little effect on their values: moving the most extreme value generally has little influence on these statistics.
On the other hand, the mean and standard deviation are more heavily influenced by changes in extreme observations, which can be important in some situations.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "robust statistics")
```

::: {.workedexample}
The median and IQR did not change under the three scenarios in Table \@ref(tab:robustOrNotTable).
Why might this be the case?

------------------------------------------------------------------------

The median and IQR are only sensitive to numbers near $Q_1,$ the median, and $Q_3.$ Since values in these regions are stable in the three data sets, the median and IQR estimates are also stable.
:::

::: {.guidedpractice}
The distribution of loan amounts in the `loan50` data set is right skewed, with a few large loans lingering out into the right tail.
If you were wanting to understand the typical loan size, should you be more interested in the mean or median?[^summarizing-visualizing-data-13]
:::

[^summarizing-visualizing-data-13]: If we are looking to simply understand what a typical individual loan looks like, the median is probably more useful.
    However, if the goal is to understand something that scales well, such as the total amount of money we might need to have on hand if we were to offer 1,000 loans, then the mean would be more useful.

### Transforming data {#transforming-data}

When data are very strongly skewed, we sometimes transform them so they are easier to model.

```{r county-pop-transform, fig.cap="Plot A: A histogram of the populations of all US counties. Plot B: A histogram of log$_{10}$-transformed county populations. For the right plot, the x-value corresponds to the power of 10, e.g., 4 on the x-axis corresponds to $10^4 =$ 10,000. Data are from 2017.", warning = FALSE}

p_pop <- ggplot(county, aes(x = pop2017)) +
  geom_histogram(binwidth = 500000, fill = COL["blue", "full"], color = "#4287aa") +
  scale_x_continuous(labels = label_number(scale = 0.000001, suffix = "M", accuracy = 1)) +
  labs(
    x = "Population (M = millions)", 
    y = "Frequency"
    )

p_pop_log <- ggplot(county, aes(x = log(pop2017, base = 10))) +
  geom_histogram(binwidth = 0.5, fill = COL["blue", "full"], color = "#4287aa") +
  labs(
    x = expression(log[10]*"(Population)"), 
    y = "Frequency"
    )

p_pop + p_pop_log +
  plot_annotation(tag_levels = "A") & 
  theme(plot.tag = element_text(size = 10))
```

::: {.workedexample}
Consider the histogram of county populations shown on the left in Figure \@ref(fig:county-pop-transform), which shows extreme skew\index{skew!extreme}.
What characteristics of the plot keep it from being useful?

------------------------------------------------------------------------

Nearly all of the data fall into the left-most bin, and the extreme skew obscures many of the potentially interesting details at the low values.
:::

There are some standard transformations that may be useful for strongly right skewed data where much of the data is positive but clustered near zero.
A **transformation** is a rescaling of the data using a function.
For instance, a plot of the logarithm (base 10) of county populations results in the new histogram on the right in Figure \@ref(fig:county-pop-transform).
The transformed data is symmetric, and any potential outliers appear much less extreme than in the original data set.
By reigning in the outliers and extreme skew, transformations often make it easier to build statistical models for the data.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "transformation")
```

Transformations can also be applied to one or both variables in a scatterplot.
A scatterplot of the population change from 2010 to 2017 against the population in 2010 is shown in Figure \@ref(fig:county-pop-change-transform).
In this first scatterplot, it's hard to decipher any interesting patterns because the population variable is so strongly skewed (left plot).
However, if we apply a log$_{10}$ transformation to the population variable, as shown in Figure \@ref(fig:county-pop-change-transform), a positive association between the variables is revealed (right plot).
In fact, we may be interested in fitting a trend line to the data when we explore methods around fitting regression lines in Chapter \@ref(intro-linear-models).

```{r county-pop-change-transform, fig.cap="Plot A: Scatterplot of population change against the population before the change. Plot B: A scatterplot of the same data but where the population size has been log-transformed.", warning = FALSE}

p_pop_change <- ggplot(county, aes(y = pop_change, x = pop2010)) +
  geom_point() +
  scale_x_continuous(labels = label_number(scale = 0.000001, suffix = "M", accuracy = 1)) +
  labs(
    x = "Population before change (M = millions)", 
    y = "Population change"
    )

p_pop_change_log <- ggplot(county, aes(y = pop_change, x = log(pop2010, base = 10))) +
  geom_point() +
  labs(
    x = expression(log[10]*"(Population before change)"), 
    y = "Population change"
    )

p_pop_change + p_pop_change_log +
  plot_annotation(tag_levels = "A") & 
  theme(plot.tag = element_text(size = 10))
```

Transformations other than the logarithm can be useful, too.
For instance, the square root $(\sqrt{\text{original observation}})$ and inverse $\bigg ( \frac{1}{\text{original observation}} \bigg )$ are commonly used by data scientists.
Common goals in transforming data are to see the data structure differently, reduce skew, assist in modeling, or straighten a nonlinear relationship in a scatterplot.

### Mapping data

\index{intensity map}

The `county` data set offers many numerical variables that we could plot using dot plots, scatterplots, or box plots, but they can miss the true nature of the data as geographic.
When we encounter geographic data, we should create an **intensity map**, where colors are used to show higher and lower values of a variable.
Figures \@ref(fig:county-intensity-map-poverty-unemp) and \@ref(fig:county-intensity-map-howownership-median-income) show intensity maps for poverty rate in percent (`poverty`), unemployment rate in percent (`unemployment_rate`), homeownership rate in percent (`homeownership`), and median household income in \$1000s (`median_hh_income`).
The color key indicates which colors correspond to which values.
The intensity maps are not generally very helpful for getting precise values in any given county, but they are very helpful for seeing geographic trends and generating interesting research questions or hypotheses.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "intensity map")
```

::: {.workedexample}
What interesting features are evident in the poverty and unemployment rate intensity maps?

------------------------------------------------------------------------

Poverty rates are evidently higher in a few locations.
Notably, the deep south shows higher poverty rates, as does much of Arizona and New Mexico.
High poverty rates are evident in the Mississippi flood plains a little north of New Orleans and also in a large section of Kentucky.

The unemployment rate follows similar trends, and we can see correspondence between the two variables.
In fact, it makes sense for higher rates of unemployment to be closely related to poverty rates.
One observation that stand out when comparing the two maps: the poverty rate is much higher than the unemployment rate, meaning while many people may be working, they are not making enough to break out of poverty.
:::

::: {.guidedpractice}
What interesting features are evident in the median household income intensity map in Figure \@ref(fig:county-intensity-map-howownership-median-income)?[^summarizing-visualizing-data-14]
:::

[^summarizing-visualizing-data-14]: Note: answers will vary.
    There is some correspondence between high earning and metropolitan areas, where we can see darker spots (higher median household income), though there are several exceptions.
    You might look for large cities you are familiar with and try to spot them on the map as dark spots.

```{r county-map-prep}
dfips <- maps::county.fips %>%
  as_tibble() %>% 
  extract(polyname, c("region", "subregion"), "^([^,]+),([^,]+)$")

map_county <- map_data("county") %>%
  as_tibble() %>%
  left_join(dfips) %>%
  mutate(fips = case_when(
    subregion == "okaloosa"  & region == "florida"        ~ 12091L,
    subregion == "st martin" & region == "louisiana"      ~ 22099L,
    subregion == "currituck" & region == "north carolina" ~ 37053L,
    # Oglala Lakota Count, see https://en.wikipedia.org/wiki/Oglala_Lakota_County,_South_Dakota
    subregion == "shannon"   & region == "south dakota"   ~ 46113L, 
    subregion == "galveston" & region == "texas"          ~ 48167L,
    subregion == "accomack"  & region == "virginia"       ~ 51001L,
    subregion == "pierce"    & region == "washington"     ~ 53053L,
    subregion == "san juan"  & region == "washington"     ~ 53055L,
    TRUE ~ fips
  ))

county_for_map <- county_complete %>%
  select(fips, name, state, poverty_2017, unemployment_rate_2017, homeownership_2010, median_household_income_2017)

map_county <- map_county %>%
  left_join(county_for_map, by = "fips")
```

```{r county-intensity-map-poverty-unemp, fig.cap = "Plot A: Intensity map of poverty rate (percent). Plot B: Intensity map of the unemployment rate (percent).", fig.width = 10, fig.asp = 1}
map_poverty <- ggplot(map_county, aes(x = long, y = lat, group = group)) +
  geom_polygon(aes(fill = poverty_2017)) +
  scale_fill_viridis_c(option = "D", labels = label_percent(scale = 1)) +
  labs(x = NULL, y = NULL, fill = "Poverty") +
  theme(axis.title.x = element_blank(),
        axis.text.x  = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())

map_unemp <- ggplot(map_county, aes(x = long, y = lat, group = group)) +
  geom_polygon(aes(fill = unemployment_rate_2017)) +
  scale_fill_viridis_c(option = "A", labels = label_percent(scale = 1, accuracy = 1)) +
  labs(x = NULL, y = NULL, fill = "Unemployment\nrate") +
  theme(axis.title.x = element_blank(),
        axis.text.x  = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())

map_poverty / 
  map_unemp +
  plot_annotation(tag_levels = "A") & 
  theme(plot.tag = element_text(size = 10))
```

```{r county-intensity-map-howownership-median-income, fig.cap = "Plot A: Intensity map of homeownership rate (percent). Plot B: Intensity map of median household income ($1000s).", fig.width = 10, fig.asp = 1}
map_howownership <- ggplot(map_county, aes(x = long, y = lat, group = group)) +
  geom_polygon(aes(fill = homeownership_2010)) +
  scale_fill_viridis_c(option = "B", labels = label_percent(scale = 1)) +
  labs(x = NULL, y = NULL, fill = "Homeownership\nrate") +
  theme(axis.title.x = element_blank(),
        axis.text.x  = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) 

map_median_income <- ggplot(map_county, aes(x = long, y = lat, group = group)) +
  geom_polygon(aes(fill = median_household_income_2017)) +
  scale_fill_viridis_c(option = "C", labels = label_dollar(scale = 0.001, suffix = "K")) +
  labs(x = NULL, y = NULL, fill = "Median\nhousehold\nincome") +
  theme(axis.title.x = element_blank(),
        axis.text.x  = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())

map_howownership / 
  map_median_income +
  plot_annotation(tag_levels = "A") & 
  theme(plot.tag = element_text(size = 10))
```

### Exercises {#explore-numerical-data-exercises}

::: {.sectionexercise}
```{r intro, child="02/exercises/02-01-numerical-data.Rmd"}
```
:::

## Exploring categorical data {#categorical-data}

In this section, we will introduce tables and other basic tools for categorical data that are used throughout this book.
The `loan50` data set represents a sample from a larger loan data set called `loans`.
This larger data set contains information on 10,000 loans made through Lending Club.
We will examine the relationship between `homeownership`, which for the `loans` data can take a value of `rent`, `mortgage` (owns but has a mortgage), or `own`, and `app_type`, which indicates whether the loan application was made with a partner or whether it was an individual application.

::: {.data}
The data can be found in the [openintro](http://openintrostat.github.io/openintro) package: [`loans_full_schema`](http://openintrostat.github.io/openintro/reference/loans_full_schema.html).
:::

### Contingency tables and bar plots

```{r}
loans_full_schema <- loans_full_schema %>%
  mutate(application_type = as.character(application_type)) %>%
  filter(application_type != "") %>%
  mutate(
    homeownership    = tolower(homeownership), 
    homeownership    = fct_relevel(homeownership, "rent", "mortgage", "own"), 
    application_type = fct_relevel(application_type, "joint", "individual")
    ) 

loans_individual_rent <- loans_full_schema %>%
  filter(
    application_type == "individual",
    homeownership    == "rent") %>%
  nrow()
```

Table \@ref(tab:loan-home-app-type-totals) summarizes two variables: `application_type` and `homeownership`.
A table that summarizes data for two categorical variables in this way is called a **contingency table**.
Each value in the table represents the number of times a particular combination of variable outcomes occurred.
For example, the value `r loans_individual_rent` corresponds to the number of loans in the data set where the borrower rents their home and the application type was by an individual.
Row and column totals are also included.
The **row totals** provide the total counts across each row and the **column totals** down each column.
We can also create a table that shows only the overall percentages or proportions for each combination of categories, or we can create a table for a single variable, such as the one shown in Table \@ref(tab:loan-homeownership-totals) for the `homeownership` variable.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "contingency table", "row totals", "column totals")
```

```{r loan-home-app-type-totals}
loans_full_schema %>%
  count(application_type, homeownership) %>%
  pivot_wider(names_from = homeownership, values_from = n) %>% 
  select(application_type, mortgage, own, rent) %>%
  adorn_totals(where = c("row", "col")) %>%
  kable(caption = "A contingency table for application type and homeownership.") %>%
  kable_styling(full_width = FALSE) %>%
  add_header_above(c(" " = 1, "homeownership" = 3, " " = 1))
```

```{r loan-homeownership-totals}
loans_full_schema %>%
  mutate(homeownership = tolower(homeownership)) %>%
  count(homeownership, name = "Count") %>%
  adorn_totals(where = "row") %>%
  kable(caption = "A table summarizing the frequencies for each value of the homeownership variable: mortgage, own, and rent.") %>%
  kable_styling(full_width = FALSE)
```

A bar plot is a common way to display a single categorical variable.
The left panel of Figure \@ref(fig:loan-homeownership-bar-plot) shows a **bar plot** for the `homeownership` variable.
In the right panel, the counts are converted into proportions, showing the proportion of observations that are in each level.

```{r loan-homeownership-bar-plot, fig.cap = "Two bar plots: the left panel shows the counts and the right panel shows the proportions of values of the homeownership variable.", fig.asp=0.5, fig.width=8}
p_count <- ggplot(loans_full_schema, aes(x = homeownership)) +
  geom_bar(fill = COL[1,1]) + 
  labs(x = "Homeownership", y = "Frequency")

p_proportion <- loans_full_schema %>%
  count(homeownership) %>%
  mutate(proportion = n / sum(n)) %>%
  ggplot(aes(x = homeownership, y = proportion)) +
  geom_col(fill = COL[1,1]) + 
  labs(x = "Homeownership", y = "Proportion")

p_count + p_proportion
```

### Using a bar plot with two variables

We can display the distributions of two categorical variables on a bar plot concurrently.
Such plots are generally useful for visualizing the relationship between two categorical variables.
Figure \@ref(fig:loan-homeownership-app-type-bar-plot) shows three such plots that visualize the relationship between `homeownership` and `application_type` variables.
Plot A in Figure \@ref(fig:loan-homeownership-app-type-bar-plot) is a **stacked bar plot**.
This plot most clearly displays that loan applicants most commonly live in mortgaged homes.
It is difficult to say, based on this plot alone, how different application types vary across the levels of homeownership.
Plot B is a **dodged bar plot**.
This plot most clearly displays that within each level of homeownership, individual applications are more common than joint applications.
Finally, plot C is a **standardized bar plot** (also known as **filled bar plot**).
This plot most clearly displays that joint applications are most common among applications who live in mortgaged homes, compared to renters and owners.
This type of visualization is helpful in understanding the fraction of individual or joint loan applications for borrowers in each level of `homeownership`.
Additionally, since the proportions of joint and individual loans vary across the groups, we can conclude that the two variables are associated for this sample.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "stacked bar plot", "dodged bar plot", "filled bar plot", "standardized bar plot")
```

```{r loan-homeownership-app-type-bar-plot, fig.cap = "Three bar plots (stacked, dodged, and standardized) displaying homeownership and application type variables.", fig.asp=1, fig.width=8}
p_stacked <- ggplot(loans_full_schema, aes(x = homeownership, fill = application_type)) +
  geom_bar() +
  scale_fill_manual(values = c(COL["yellow","full"], COL["blue","full"])) + 
  labs(x = "Homeownership", y = "Frequency", fill = "Application type")

p_dodged <- ggplot(loans_full_schema, aes(x = homeownership, fill = application_type)) +
  geom_bar(position = "dodge") +
  scale_fill_manual(values = c(COL["yellow","full"], COL["blue","full"])) + 
  labs(x = "Homeownership", y = "Frequency", fill = "Application type")

p_standardized <- ggplot(loans_full_schema, aes(x = homeownership, fill = application_type)) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = c(COL["yellow","full"], COL["blue","full"])) + 
  labs(x = "Homeownership", y = "Proportion", fill = "Application type")

p_stacked + p_dodged + p_standardized + guide_area() +
  plot_annotation(tag_levels = "A") +
  plot_layout(guides = "collect")
```

::: {.workedexample}
Examine the three bar plots in Figure \@ref(fig:loan-homeownership-app-type-bar-plot).
When is the stacked, dodged, or standardized bar plot the most useful?

------------------------------------------------------------------------

The stacked bar plot is most useful when it's reasonable to assign one variable as the explanatory variable (here `homeownership`) and the other variable as the response (here `application_type`), since we are effectively grouping by one variable first and then breaking it down by the others.

Dodged bar plots are more agnostic in their display about which variable, if any, represents the explanatory and which the response variable.
It is also easy to discern the number of cases in of the six different group combinations.
However, one downside is that it tends to require more horizontal space; the narrowness of Plot B compared to the other two in Figure \@ref(fig:loan-homeownership-app-type-bar-plot) makes the plot feel a bit cramped.
Additionally, when two groups are of very different sizes, as we see in the group `own` relative to either of the other two groups, it is difficult to discern if there is an association between the variables.

The standardized stacked bar plot is helpful if the primary variable in the stacked bar plot is relatively imbalanced, e.g., the category has only a third of the observations in the category, making the simple stacked bar plot less useful for checking for an association.
The major downside of the standardized version is that we lose all sense of how many cases each of the bars represents.
:::

### Mosaic plots

A **mosaic plot** is a visualization technique suitable for contingency tables that resembles a standardized stacked bar plot with the benefit that we still see the relative group sizes of the primary variable as well.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "mosaic plot")
```

To get started in creating our first mosaic plot, we'll break a square into columns for each category of the variable, with the result shown in Plot A of Figure \@ref(fig:loan-homeownership-type-mosaic-plot).
Each column represents a level of `homeownership`, and the column widths correspond to the proportion of loans in each of those categories.
For instance, there are fewer loans where the borrower is an owner than where the borrower has a mortgage.
In general, mosaic plots use box *areas* to represent the number of cases in each category.

Plot B in Figure \@ref(fig:loan-homeownership-type-mosaic-plot) displays the relationship between homeownership and application type.
Each column is split proportionally to the number of loans from individual and joint borrowers.
For example, the second column represents loans where the borrower has a mortgage, and it was divided into individual loans (upper) and joint loans (lower).
As another example, the bottom segment of the third column represents loans where the borrower owns their home and applied jointly, while the upper segment of this column represents borrowers who are homeowners and filed individually.
We can again use this plot to see that the `homeownership` and `application_type` variables are associated, since some columns are divided in different vertical locations than others, which was the same technique used for checking an association in the standardized stacked bar plot.

```{r loan-homeownership-type-mosaic-plot, fig.cap = "The mosaic plots: one for homeownership alone and the other displaying the relationship between homeownership and application type.", fig.asp=0.5, fig.width=8}
p_mosaic_1 <- ggplot(loans_full_schema) +
  geom_mosaic(aes(x = product(homeownership))) +
  labs(x = "Homeownership", y = "") +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank())

p_mosaic_2 <- ggplot(loans_full_schema) +
  geom_mosaic(aes(x = product(homeownership), fill = application_type)) +
  scale_fill_manual(values = c(COL["yellow","full"], COL["blue","full"])) +
  labs(x = "Homeownership", y = "") +
  guides(fill = FALSE)

p_mosaic_1 + p_mosaic_2 +
  plot_annotation(tag_levels = "A")
```

In Figure \@ref(fig:loan-homeownership-type-mosaic-plot), we chose to first split by the homeowner status of the borrower.
However, we could have instead first split by the application type, as in Figure \@ref(fig:loan-app-type-mosaic-plot).
Like with the bar plots, it's common to use the explanatory variable to represent the first split in a mosaic plot, and then for the response to break up each level of the explanatory variable, if these labels are reasonable to attach to the variables under consideration.

```{r loan-app-type-mosaic-plot, fig.cap = "Mosaic plot where loans are grouped by homeownership after they have been divided into individual and joint application types.", fig.asp=1}
ggplot(loans_full_schema) +
  geom_mosaic(aes(x = product(application_type), fill = homeownership)) +
  scale_fill_manual(values = c(COL["blue","full"], COL["gray","full"], COL["red","full"])) +
  labs(x = "Application type", y = "") +
  guides(fill = FALSE)
```

### Row and column proportions

In the previous sections we inspected visualisations of two categorical variables in bar plots and mosaic plots.
However, we have not discussed how the values in the bar and mosaic plots that show proportions are calculated.
In this section we will investigate fractional breakdown of one variable in another and we can modify our contingency table to provide such a view.
Table \@ref(tab:loan-home-app-type-row-proportions) shows **row proportions** for Table \@ref(tab:loan-home-app-type-totals), which are computed as the counts divided by their row totals.
The value 3496 at the intersection of individual and rent is replaced by $3496 / 8505 = 0.411,$ i.e., 3496 divided by its row total, 8505.
So what does 0.411 represent?
It corresponds to the proportion of individual applicants who rent.

```{r loan-home-app-type-row-proportions}
loans_full_schema %>%
  count(application_type, homeownership) %>% 
  group_by(application_type) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>%
  pivot_wider(names_from = homeownership, values_from = proportion) %>% 
  adorn_totals(where = "col") %>% 
  kable(caption = "A contingency table with row proportions for the application type and homeownership variables.") %>%
  kable_styling(full_width = FALSE) %>%
  add_header_above(c(" " = 1, "homeownership" = 3, " " = 1))
```

A contingency table of the column proportions is computed in a similar way, where each is computed as the count divided by the corresponding column total.
Table \@ref(tab:loan-home-app-type-column-proportions) shows such a table, and here the value 0.906 indicates that 90.6% of renters applied as individuals for the loan.
This rate is higher compared to loans from people with mortgages (80.2%) or who own their home (86.5%).
Because these rates vary between the three levels of `homeownership` (`rent`, `mortgage`, `own`), this provides evidence that `app_type` and `homeownership` variables may be associated.

```{r loan-home-app-type-column-proportions}
loans_full_schema %>%
  count(application_type, homeownership) %>% 
  group_by(homeownership) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>%
  pivot_wider(names_from = homeownership, values_from = proportion) %>% 
  adorn_totals(where = "row") %>% 
  kable(caption = "A contingency table with column proportions for the application type and homeownership variables.") %>%
  kable_styling(full_width = FALSE) %>%
  add_header_above(c(" " = 1, "homeownership" = 3))
```

We could also have checked for an association between `application_type` and `homeownership` in Table \@ref(tab:loan-home-app-type-row-proportions) using row proportions.
When comparing these row proportions, we would look down columns to see if the fraction of loans where the borrower rents, has a mortgage, or owns varied across the to application types.

::: {.guidedpractice}
(a) What does 0.451 represent in Table \@ref(tab:loan-home-app-type-row-proportions)?

(b) What does 0.802 represent in Table \@ref(tab:loan-home-app-type-column-proportions)?[^summarizing-visualizing-data-15]
:::

[^summarizing-visualizing-data-15]: (a) 0.451 represents the proportion of individual applicants who have a mortgage.
    (b) 0.802 represents the fraction of applicants with mortgages who applied as individuals.

::: {.guidedpractice}
(a) What does 0.122 represent in Table \@ref(tab:loan-home-app-type-row-proportions)?

(b) What does 0.135 represent in Table \@ref(tab:loan-home-app-type-column-proportions)?[^summarizing-visualizing-data-16]
:::

[^summarizing-visualizing-data-16]: (a) 0.122 represents the fraction of joint borrowers who own their home.
    (b) 0.135 represents the home-owning borrowers who had a joint application for the loan.

::: {.workedexample}
Data scientists use statistics to filter spam from incoming email messages.
By noting specific characteristics of an email, a data scientist may be able to classify some emails as spam or not spam with high accuracy.
One such characteristic is whether the email contains no numbers, small numbers, or big numbers.
Another characteristic is the email format, which indicates whether or not an email has any HTML content, such as bolded text.
We'll focus on email format and spam status using the data set; these variables are summarized in a contingency table in Table \@ref(tab:emailSpamHTMLTableTotals).
Which would be more helpful to someone hoping to classify email as spam or regular email for this table: row or column proportions?

------------------------------------------------------------------------

A data scientist would be interested in how the proportion of spam changes within each email format.
This corresponds to column proportions: the proportion of spam in plain text emails and the proportion of spam in HTML emails.

If we generate the column proportions, we can see that a higher fraction of plain text emails are spam ($209/1195 = 17.5\%$) than compared to HTML emails ($158/2726 = 5.8\%$).
This information on its own is insufficient to classify an email as spam or not spam, as over 80% of plain text emails are not spam.
Yet, when we carefully combine this information with many other characteristics, we stand a reasonable chance of being able to classify some emails as spam or not spam with confidence.
This example points out that row and column proportions are not equivalent.
Before settling on one form for a table, it is important to consider each to ensure that the most useful table is constructed.
However, sometimes it simply isn't clear which, if either, is more useful.
:::

```{r emailSpamHTMLTableTotals}
email %>%
  mutate(
    format = if_else(format == 0, "text", "HTML"), 
    spam   = if_else(spam == 0, "not spam", "spam"),  
  ) %>% 
  count(spam, format) %>%
  pivot_wider(names_from = format, values_from = n) %>% 
  adorn_totals(where = c("row", "col")) %>%
  kable(caption = "A contingency table for spam and format") 
```

::: {.workedexample}
Look back to Table \@ref(tab:loan-home-app-type-row-proportions) and Table \@ref(tab:loan-home-app-type-column-proportions).
Are there any obvious scenarios where one might be more useful than the other?

------------------------------------------------------------------------

None that we think are obvious!
What is distinct about and vs the email example is that the two loan variables don't have a clear explanatory-response variable relationship that we might hypothesize.
Usually it is most useful to "condition" on the explanatory variable.
For instance, in the email example, the email format was seen as a possible explanatory variable of whether the message was spam, so we would find it more interesting to compute the relative frequencies (proportions) for each email format.
:::

### Pie charts

A **pie chart** is shown in Figure \@ref(fig:loan-homeownership-pie-chart) alongside a bar plot representing the same information.
Pie charts can be useful for giving a high-level overview to show how a set of cases break down.
However, it is also difficult to decipher details in a pie chart.
For example, it;s not immediately obvious that there are more loans where the borrower has a mortgage than rent when looking at the pie chart, while this detail is very obvious in the bar plot.
While pie charts can be useful, we prefer bar plots for their ease in comparing groups.

```{r loan-homeownership-pie-chart, fig.cap="A pie chart of homeownership."}
loans_full_schema %>% 
  count(homeownership) %>%
  ggplot(aes(x = "", fill = homeownership, y = n)) + 
  geom_bar(stat = "identity") +
  coord_polar("y", start = 0) +
  scale_fill_manual(values = c(COL["blue","full"], COL["gray","full"], COL["red","full"])) +
  theme_void()
```

### Comparing numerical data across groups

Some of the more interesting investigations can be considered by examining numerical data across groups.
In this section we will expand on a few methods we've already seen to make plots for numerical data from multiple groups on the same graph as well as introduce a few new methods for comparing numerical data across groups.

We will revisit the `county` dataset and compare the median household income for counties that gained population from 2010 to 2017 versus counties that had no gain.
While we might like to make a causal connection between income and population growth, remember that these are observational data and so such an interpretation would be, at best, half-baked.

```{r}
n_county          <- nrow(county)
n_missing_pop2017 <- county %>% 
  filter(is.na(pop2017)) %>% 
  nrow()
n_with_pop_change <- n_county - n_missing_pop2017

county <- county %>%
  mutate(
    pop_change_3levels = case_when(
      pop_change < 0 ~ "loss",
      pop_change == 0 ~ "no change",
      pop_change > 0 ~ "gain"
    ),
    pop_change_2levels = if_else(pop_change_3levels == "gain", "gain", "no gain")
  )

n_pop_no_gain     <- county %>% filter(pop_change_2levels == "no gain") %>% nrow()
n_pop_loss        <- county %>% filter(pop_change_3levels == "loss") %>% nrow()
n_pop_no_change   <- county %>% filter(pop_change_3levels == "no change") %>% nrow()
n_pop_gain        <- county %>% filter(pop_change_3levels == "gain") %>% nrow()
```

We have data on `r n_county` counties in the United States.
We are missing 2017 population data from `r n_missing_pop2017` of them, and of the remaining `r n_with_pop_change` counties, in `r n_pop_gain` the population increased from 2010 to 2017 and in the remaining `r n_pop_no_gain` the population decreased.
Table \@ref(tab:countyIncomeSplitByPopGainTable) shows a sample of 5 observations from each group.

```{r countyIncomeSplitByPopGainTable}
county %>%
  select(state, name, pop_change, pop_change_2levels, median_hh_income) %>%
  filter(!is.na(pop_change)) %>%
  group_by(pop_change_2levels) %>% 
  slice_sample(n = 5) %>%
  arrange(pop_change_2levels, state, name) %>%
  rename(
    State = state,
    County = name,
    `Change in population, %` = pop_change,
    `Gain / No gain` = pop_change_2levels,
    `Median household income` = median_hh_income
  ) %>%
  kable(caption = "The median household income from a random sample of 5 counties with population gain between 2010 to 2017 and another random sample of 5 counties with no population gain.")
```

Color can be used to split histograms for numerical variables by levels of a categorical variable.
An example of this is shown in Plot A of Figure \@ref(fig:countyIncomeSplitByPopGain).
The **side-by-side box plot** is another traditional tool for comparing across groups.
An example is shown in Plot B of Figure \@ref(fig:countyIncomeSplitByPopGain), where there are two box plots, one for each group, placed into one plotting window and drawn on the same scale.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "side-by-side box plot")
```

```{r countyIncomeSplitByPopGain, fig.cap="Histograms (Plot A) and side by-side box plots (Plot B) for `med_hh_income`, where counties are split by whether there was a population gain or not.", fig.width = 8}
p_hist <- county %>%
  filter(!is.na(pop_change)) %>%
  ggplot(aes(x = median_hh_income, fill = pop_change_2levels)) +
  geom_histogram(binwidth = 5000, alpha = 0.5) +
  scale_fill_manual(values = c(COL["blue", "full"], COL["green", "full"])) +
  scale_x_continuous(labels = label_dollar()) +
  labs(x = "Median household income", y = NULL, fill = "Change\nin population")

p_box <- county %>%
  filter(!is.na(pop_change)) %>%
  ggplot(aes(x = median_hh_income, y = pop_change_2levels, color = pop_change_2levels)) +
  geom_boxplot() +
  scale_color_manual(values = c(COL["blue", "full"], COL["green", "full"])) +
  scale_x_continuous(labels = label_dollar()) +
  labs(x = "Median household income", y = NULL, color = "Change\nin population")

p_hist / p_box + plot_annotation(tag_levels = "A")
```

::: {.guidedpractice}
Use the plots in Figure \@ref(fig:countyIncomeSplitByPopGain) to compare the incomes for counties across the two groups.
What do you notice about the approximate center of each group?
What do you notice about the variability between groups?
Is the shape relatively consistent between groups?
How many *prominent* modes are there for each group?[^summarizing-visualizing-data-17]
:::

[^summarizing-visualizing-data-17]: Answers may vary a little.
    The counties with population gains tend to have higher income (median of about \$45,000) versus counties without a gain (median of about \$40,000).
    The variability is also slightly larger for the population gain group.
    This is evident in the IQR, which is about 50% bigger in the *gain* group.
    Both distributions show slight to moderate right skew and are unimodal.
    The box plots indicate there are many observations far above the median in each group, though we should anticipate that many observations will fall beyond the whiskers when examining any data set that contain more than a few hundred data points.

::: {.guidedpractice}
What components of each plot in Figure \@ref(fig:countyIncomeSplitByPopGain) do you find most useful?[^summarizing-visualizing-data-18]
:::

[^summarizing-visualizing-data-18]: Answers will vary.
    The side-by-side box plots are especially useful for comparing centers and spreads, while the hollow histograms are more useful for seeing distribution shape, skew, modes, and potential anomalies.

Another useful visualisation for comparing numerical data across groups is a **ridge plot**, which combines density plots for various groups drawn on the same scale in a single plotting window.
Figure \@ref(fig:countyIncomeSplitByPopGainRidge) displays a ridge plot for the distribution of median household income in counties, split by whether there was a population gain or not.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "ridge plot")
```

```{r countyIncomeSplitByPopGainRidge, fig.cap="Ridge plot for for `med_hh_income`, where counties are split by whether there was a population gain or not."}
county %>%
  filter(!is.na(pop_change)) %>%
  ggplot(aes(x = median_hh_income, y = pop_change_2levels, 
             fill = pop_change_2levels, color = pop_change_2levels)) + 
  geom_density_ridges(alpha = 0.5) +
  scale_fill_manual(values = c(COL["blue", "full"], COL["green", "full"])) +
  scale_color_manual(values = c(COL["blue", "full"], COL["green", "full"])) +
  scale_x_continuous(labels = label_dollar()) +
  labs(x = "Median household income", y = NULL, fill = "Change\nin population", color = "Change\nin population")
```

::: {.guidedpractice}
What components of the ridge plot in Figure \@ref(fig:countyIncomeSplitByPopGainRidge) do you find most useful compared to those in Figure \@ref(fig:countyIncomeSplitByPopGain)?[^summarizing-visualizing-data-19]
:::

[^summarizing-visualizing-data-19]: The ridge plot give us a better sense of the shape, and especially modality, of the data.

One last visualization technique we'll highlight for comparing numerical data across groups is **faceting**.
In this technique we split (facet) the graphical display of the data across plotting windows based on groups.
Plot A in Figure \@ref(fig:countyIncomeSplitByPopGainFacetHist) displays the same information as Plot A in Figure \@ref(fig:countyIncomeSplitByPopGain), however here the distributions of median household income for counties with and without population gain are faceted across two plotting windows.
We preserve the same scale on the x and y axes for easier comparison.
An advantage of this approach is that it extends to splitting the data across levels of two categorical variables, which allows for displaying relationships between three variables.
In Plot B in Figure \@ref(fig:countyIncomeSplitByPopGainFacetHist) we have now split the data into four groups using the `pop_change` and `metro` variables:

-   top left represents counties that are *not* in a `metro`politan area with population gain,
-   top right represents counties that are in a metropolitan area with population gain,
-   bottom left represents counties that are *not* in a metropolitan area without population gain, and finally
-   bottom right represents counties that are in a metropolitan area without population gain.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "faceted plot")
```

```{r countyIncomeSplitByPopGainFacetHist, fig.cap="Distribution of median income in counties using faceted histograms: Plot A facets by whether there was a population gain or not and Plot B facets by both population gain and whether the county is in a metropolitan area.", , fig.width = 8}
p_1 <- county %>%
  filter(!is.na(pop_change) & !is.na(metro)) %>%
  # for better labelling on plot
  rename(
    pop_change_num = pop_change,
    pop_change = pop_change_2levels
    ) %>%
  ggplot(aes(x = median_hh_income, fill = pop_change)) +
  geom_histogram(binwidth = 5000) +
  scale_fill_manual(values = c(COL["blue", "full"], COL["green", "full"])) +
  scale_x_continuous(labels = label_dollar()) +
  facet_grid(pop_change~., labeller = label_both) +
  labs(x = "Median household income", y = NULL) +
  guides(fill = FALSE)

p_2 <- county %>%
  filter(!is.na(pop_change) & !is.na(metro)) %>%
  # for better labelling on plot
  rename(
    pop_change_num = pop_change,
    pop_change = pop_change_2levels
    ) %>%
  ggplot(aes(x = median_hh_income, fill = pop_change)) +
  geom_histogram(binwidth = 5000) +
  scale_fill_manual(values = c(COL["blue", "full"], COL["green", "full"])) +
  scale_x_continuous(labels = label_dollar()) +
  facet_grid(pop_change~metro, labeller = label_both) +
  labs(x = "Median household income", y = NULL) +
  guides(fill = FALSE)

p_1 + p_2 + 
  plot_annotation(tag_levels = "A") + 
  plot_layout(widths = c(1, 2))
```

We can continue building up on this visualisation to add one more variable, `median_edu`, which is the median education level in the county.
In Figure \@ref(fig:countyIncomeRidgeMulti), we represent median education level using color, where yellow (dotted line) represents counties where the median (dashed line) education level is Bachelor's, green is some college degree, and blue (solid line) is high school diploma.

```{r countyIncomeRidgeMulti, fig.cap="Distribution of median income in counties using a ridge plot, faceted by whether the county had a population gain or not as well as whether the county is in a metropolitan area and colored by the median education level in the county."}
county %>%
  filter(!is.na(pop_change) & !is.na(metro) & !is.na(median_edu) & median_edu != "below_hs") %>%
  # for better labelling on plot
  rename(
    pop_change_num = pop_change,
    pop_change = pop_change_2levels
    ) %>%
  ggplot(aes(x = median_hh_income, y = median_edu, fill = median_edu, color = median_edu)) +
  geom_density_ridges(alpha = 0.5, aes(linetype = median_edu)) +
  scale_fill_manual(values = c(COL["blue", "full"], COL["green", "full"], COL["yellow", "full"])) +
  scale_color_manual(values = c(COL["blue", "full"], COL["green", "full"], COL["yellow", "full"])) +
  scale_linetype_manual(values = c("solid", "dashed", "dotted")) +
  scale_x_continuous(labels = label_dollar()) +
  facet_grid(pop_change~metro, labeller = label_both) +
  labs(x = "Median household income", y = NULL) +
  guides(fill = FALSE, color = FALSE, linetype = FALSE)
```

::: {.guidedpractice}
Based on Figure \@ref(fig:countyIncomeRidgeMulti), what can you say about how median household income in counties vary depending on population gain/no gain, metropolitan area/not, and median degree?[^summarizing-visualizing-data-20]
:::

[^summarizing-visualizing-data-20]: The ridge plot give us a better sense of the shape, and especially modality, of the data.

### Exercises {#explore-categorical-data-exercises}

::: {.sectionexercise}
```{r intro, child="02/exercises/02-02-categorical-data.Rmd"}
```
:::

## Effective data visualization

### Keep it simple

We discussed earlier that pie charts do not tend to be useful when the categorical variable being displayed has many levels.
In addition, there is little value added to coloring each pie.
And definitely no value added to making the pie chart three dimensional.
A simple bar plot can communicate the same information in a simpler, easier to digest way.

```{r}
expenses <- tribble(
  ~category,                     ~value,
  "Cutting tools"                , 0.03,
  "Buildings and administration" , 0.22,
  "Labor"                        , 0.31,
  "Machinery"                    , 0.27,
  "Workplace materials"          , 0.17
) %>%
  mutate(value =  value * 100) %>%
  uncount(weights = value)
```

```{r out.width="30%"}
# remake in R using ggthreed
knitr::include_graphics("02/figures/pie-3d.jpg")
```

vs.

```{r pie-to-bar}
ggplot(expenses, aes(x = fct_infreq(category))) +
  geom_bar() +
  theme_minimal() +
  coord_flip() +
  labs(x = "", y = "")
```

### Use color to draw attention

Avoid adding color just to add color, instead use it to draw attention.
This doesn't mean you shouldn't think about how visually pleasing your visualization is, and if you're adding color for making it visually pleasing without drawing attention to a particular feature, that might still be fine.
But you should be critical of default coloring and explicitly decide whether to include color and how.
Also note that not everyone sees color the same way, often it's useful to add color and one more feature (e.g., pattern) so that you can refer to the features you're drawing attention to in multiple ways.

```{r fig.width = 5, fig.asp=0.8}
p_bad <- ggplot(expenses, aes(x = fct_infreq(category), fill = category)) +
  geom_bar() +
  theme_minimal() +
  coord_flip() +
  labs(x = "", y = "") +
  theme(legend.position = "none")

p_good <- ggplot(expenses, aes(x = fct_infreq(category), fill = category)) +
  geom_bar() +
  theme_minimal() +
  coord_flip() +
  labs(x = "", y = "") +
  scale_fill_manual(values = c("red", rep("gray", 4))) +
  theme(legend.position = "none")

p_bad / p_good
```

### Tell a story

```{r fig.cap = "Credit: Angela Zoss and Eric Monson, Duke Data Visualization Services", out.width="100%"}
knitr::include_graphics("02/figures/time-series.story.png")
```

### Order matters

In September 2019, YouGov survey asked 1,639 GB adults the following question[^summarizing-visualizing-data-21]:

[^summarizing-visualizing-data-21]: Source: [YouGov Survey Results](https://d25d2506sfb94s.cloudfront.net/cumulus_uploads/document/x0msmggx08/YouGov%20-%20Brexit%20and%202019%20election.pdf), retrieved Oct 7, 2019.

> In hindsight, do you think Britain was right/wrong to vote to leave EU?
>
> -   Right to leave\
> -   Wrong to leave\
> -   Don't know

```{r}
brexit <- tibble(
  opinion = c(
    rep("Right", 664), rep("Wrong", 787), rep("Don't know", 188)
    ),
  region = c(
    rep("london", 63), rep("rest_of_south", 241), rep("midlands_wales", 145), rep("north", 176), rep("scot", 39),
    rep("london", 110), rep("rest_of_south", 257), rep("midlands_wales", 152), rep("north", 176), rep("scot", 92),
    rep("london", 24), rep("rest_of_south", 49), rep("midlands_wales", 57), rep("north", 48), rep("scot", 10)
    )
)
```

Alphabetical order is rarely ideal, so sometimes it's better to order bars by frequency.

```{r}
p_bad <- ggplot(data = brexit, aes(x = opinion)) +
  geom_bar()

p_good <- ggplot(data = brexit, aes(x = fct_infreq(opinion))) +
  geom_bar()

p_bad + p_good
```

We can improve this further by cleaning up axis labels.

```{r}
ggplot(data = brexit, aes(x = opinion)) +
  geom_bar() +
  labs(x = "Opinion", y = "Count")
```

There may also be inherent ordering to levels of your categorical variables.
If so, the visualization should respect that.

```{r}
brexit <- brexit %>%
  mutate(
    region = fct_relevel(
      region,
      "london", "rest_of_south", "midlands_wales", "north", "scot"
    ),
    region = fct_recode(
      region,
      London = "london", 
      `Rest of South` = "rest_of_south", 
      `Midlands / Wales` = "midlands_wales", 
      North = "north", 
      Scotland = "scot"
    )
  )

ggplot(data = brexit, aes(x = region)) +
  geom_bar()
```

### Put long categories on the y-axis

And clean up axis labels.

```{r out.width="100%", fig.asp=0.5, fig.width=8}
p_bad <- ggplot(data = brexit, aes(x = region)) +
  geom_bar()

p_good <- ggplot(data = brexit, aes(y = region)) +
  geom_bar() +
    labs(x = "Region", y = "")

p_bad + p_good
```

### Pick a purpose

Segmented bar plots can be hard to read.
Use faceting, avoid redundancy, and use informative labels (note the shortlink to the survey).

```{r out.width="100%", fig.asp=0.8, fig.width=8}
p_bad <- ggplot(data = brexit, aes(x = region, fill = opinion)) +
  geom_bar()

p_good <- ggplot(data = brexit, aes(x = opinion)) +
  geom_bar() +
  coord_flip() +
  facet_grid(. ~ region, labeller = label_wrap_gen(width = 12)) +
  labs(
    title = "Was Britain right/wrong to vote to leave EU?",
    subtitle = "YouGov Survey Results, 2-3 September 2019",
    caption = "Source: bit.ly/2lCJZVg", 
    x = "", 
    y = ""
  )

p_bad / p_good
```

### Select meaningful colors

<!-- An example with an ordinal variable with more levels would be better. -->

Rainbow colors are not always the right choice.
Viridis scale works well with ordinal data

```{r out.width="100%", fig.asp=0.8, fig.width=8}
p_bad <- ggplot(data = brexit, aes(x = region, fill = opinion)) +
  geom_bar(position = "fill") +
  coord_flip()

p_good <- ggplot(data = brexit, aes(x = region, fill = opinion)) +
  geom_bar(position = "fill") +
  coord_flip() +
  scale_fill_viridis_d() +
  labs(
    title = "Was Britain right/wrong to vote to leave EU?",
    subtitle = "YouGov Survey Results, 2-3 September 2019",
    caption = "Source: bit.ly/2lCJZVg",
    x = "", 
    y = "",
    fill = ""
)

p_bad / p_good
```

### Exercises {#effective-dataviz-exercises}

::: {.underconstruction}
Exercises for this section will be available in the 1st edition of this book, which will be available in Summer 2021.
In the meantime, [OpenIntro::Introduction to Statistics with Randomization and Simulation](https://www.openintro.org/book/isrs/) and [OpenIntro::Statistics](https://www.openintro.org/book/os/), both of which are available for free, have many exercises you can use alongside this book.
:::

::: {.sectionexercise}
```{r intro, child="02/exercises/02-03-effective-dataviz.Rmd", eval=FALSE}
```
:::

## Case study: malaria vaccine

Suppose your professor splits the students in class into two groups: students on the left and students on the right.
If $\hat{p}_{_L}$ and $\hat{p}_{_R}$ represent the proportion of students who own an Apple product on the left and right, respectively, would you be surprised if $\hat{p}_{_L}$ did not exactly equal $\hat{p}_{_R}$?
While the proportions would probably be close to each other, it would be unusual for them to be exactly the same.
We would probably observe a small difference due to chance.

If we don't think the side of the room a person sits on in class is related to whether the person owns an Apple product, what assumption are we making about the relationship between these two variables?

### Variability within data

We consider a study on a new malaria vaccine called PfSPZ.
In this study, volunteer patients were randomized into one of two experiment groups: 14 patients received an experimental vaccine and 6 patients received a placebo vaccine.
Nineteen weeks later, all 20 patients were exposed to a drug-sensitive malaria virus strain; the motivation of using a drug-sensitive strain of virus here is for ethical considerations, allowing any infections to be treated effectively.
The results are summarized in Table \@ref(tab:malaria-vaccine-20-ex-summary), where 9 of the 14 treatment patients remained free of signs of infection while all of the 6 patients in the control group patients showed some baseline signs of infection.

```{r malaria-vaccine-20-ex-summary}
malaria %>%
  count(treatment, outcome, .drop = FALSE) %>%
  pivot_wider(names_from = outcome, values_from = n) %>%
  adorn_totals(where = c("row", "col")) %>%
  kable(caption = "Summary results for the malaria vaccine experiment.")
```

::: {.guidedpractice}
Is this an observational study or an experiment?
What implications does the study type have on what can be inferred from the results?[^summarizing-visualizing-data-22]
:::

[^summarizing-visualizing-data-22]: The study is an experiment, as patients were randomly assigned an experiment group.
    Since this is an experiment, the results can be used to evaluate a causal relationship between the malaria vaccine and whether patients showed signs of an infection.

In this study, a smaller proportion of patients who received the vaccine showed signs of an infection (35.7% versus 100%).
However, the sample is very small, and it is unclear whether the difference provides *convincing evidence* that the vaccine is effective.

::: {.workedexample}
Statisticians and data scientists are sometimes called upon to evaluate the strength of evidence.
When looking at the rates of infection for patients in the two groups in this study, what comes to mind as we try to determine whether the data show convincing evidence of a real difference?

------------------------------------------------------------------------

The observed infection rates (35.7% for the treatment group versus 100% for the control group) suggest the vaccine may be effective.
However, we cannot be sure if the observed difference represents the vaccine's efficacy or is just from random chance.
Generally there is a little bit of fluctuation in sample data, and we wouldn't expect the sample proportions to be *exactly* equal, even if the truth was that the infection rates were independent of getting the vaccine.
Additionally, with such small samples, perhaps it's common to observe such large differences when we randomly split a group due to chance alone!
:::

This example is a reminder that the observed outcomes in the data sample may not perfectly reflect the true relationships between variables since there is **random noise**.
While the observed difference in rates of infection is large, the sample size for the study is small, making it unclear if this observed difference represents efficacy of the vaccine or whether it is simply due to chance.
We label these two competing claims, $H_0$ and $H_A,$ which are spoken as "H-nought" and "H-A":

-   $H_0$: **Independence model.** The variables and are independent.
    They have no relationship, and the observed difference between the proportion of patients who developed an infection in the two groups, 64.3%, was due to chance.

-   $H_A$: **Alternative model.** The variables are *not* independent.
    The difference in infection rates of 64.3% was not due to chance.
    Here (because an experiment was done), if the difference in infection rate is not due to change, it was the vaccine that affected the rate of infection.

```{r include=FALSE}
terms_chp_2 <- c(terms_chp_2, "random noise")
```

What would it mean if the independence model, which says the vaccine had no influence on the rate of infection, is true?
It would mean 11 patients were going to develop an infection *no matter which group they were randomized into*, and 9 patients would not develop an infection *no matter which group they were randomized into*.
That is, if the vaccine did not affect the rate of infection, the difference in the infection rates was due to chance alone in how the patients were randomized.

Now consider the alternative model: infection rates were influenced by whether a patient received the vaccine or not.
If this was true, and especially if this influence was substantial, we would expect to see some difference in the infection rates of patients in the groups.

We choose between these two competing claims by assessing if the data conflict so much with $H_0$ that the independence model cannot be deemed reasonable.
If this is the case, and the data support $H_A,$ then we will reject the notion of independence and conclude the vaccine was effective.

### Simulating the study

We're going to implement **simulation** under the setting where we will pretend we know that the malaria vaccine being tested does *not* work.
Ultimately, we want to understand if the large difference we observed in the data is common in these simulations that represent independence.
If it is common, then maybe the difference we observed was purely due to chance.
If it is very uncommon, then the possibility that the vaccine was helpful seems more plausible.

Table \@ref(tab:malaria-vaccine-20-ex-summary) shows that 11 patients developed infections and 9 did not.
For our simulation, we will suppose the infections were independent of the vaccine and we were able to *rewind* back to when the researchers randomized the patients in the study.
If we happened to randomize the patients differently, we may get a different result in this hypothetical world where the vaccine doesn't influence the infection.
Let's complete another **randomization** using a simulation.

In this **simulation**, we take 20 notecards to represent the 20 patients, where we write down "infection" on 11 cards and "no infection" on 9 cards.
In this hypothetical world, we believe each patient that got an infection was going to get it regardless of which group they were in, so let's see what happens if we randomly assign the patients to the treatment and control groups again.
We thoroughly shuffle the notecards and deal 14 into a pile and 6 into a pile.
Finally, we tabulate the results, which are shown in Table \@ref(tab:malaria-vaccine-20-ex-summary-rand-1).

```{r malaria-vaccine-20-ex-summary-rand-1}
# matching OS4
malaria_rand <- tibble(
  treatment = c(rep("infection", 11), 
                rep("no infection", 9)),
  outcome   = c(rep("vaccine", 7), rep("placebo", 4),
                rep("vaccine", 7), rep("placebo", 2))
)

malaria_rand %>%
  count(treatment, outcome, .drop = FALSE) %>%
  pivot_wider(names_from = outcome, values_from = n) %>%
  adorn_totals(where = c("row", "col")) %>%
  kable(caption = "Simulation results, where any difference in infection ratio is purely due to chance.")
```

::: {.guidedpractice}
How does this compare to the observed 64.3% difference in the actual data?[^summarizing-visualizing-data-23]
:::

[^summarizing-visualizing-data-23]: $4 / 6 - 7 / 14 = 0.167$ or about 16.7% in favor of the vaccine.
    This difference due to chance is much smaller than the difference observed in the actual groups.

### Checking for independence

We computed one possible difference under the independence model in the previous Guided Practice, which represents one difference due to chance.
While in this first simulation, we physically dealt out notecards to represent the patients, it is more efficient to perform the simulation using a computer.

Repeating the simulation on a computer, we get another difference due to chance: $$ \frac{2}{6{}} - \frac{9}{14{}} = -0.310 $$

And another: $$ \frac{3}{6{}} - \frac{8}{14{}} = -0.071$$

And so on until we repeat the simulation enough times to create a *distribution of differences from chance alone*.

Figure \@ref(fig:malaria-rand-dot-plot) shows a stacked plot of the differences found from 100 simulations, where each dot represents a simulated difference between the infection rates (control rate minus treatment rate).

```{r malaria-rand-dot-plot, fig.cap = "A stacked dot plot of differences from 100 simulations produced under the independence mode, $H_0,$ where in these simulations infections are unaffected by the vaccine. Two of the 100 simulations had a difference of at least 64.3%, the difference observed in the study."}
set.seed(19)
malaria %>%
  specify(response = outcome, explanatory = treatment, success = "infection") %>%
  hypothesize(null = "independence") %>%
  generate(reps = 100, type = "permute") %>%
  calculate(stat = "diff in props", order = c("placebo", "vaccine")) %>%
  # simplify by rounding
  mutate(stat = round(stat, 3)) %>%
  ggplot(aes(x = stat)) +
  geom_dotplot(binwidth = 0.1, dotsize = 0.2, fill = COL["blue", "full"], color = COL["blue", "full"]) +
  labs(y = NULL, x = "Difference in infection rates") +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank()) +
  gghighlight(stat >= 0.643)
```

Note that the distribution of these simulated differences is centered around 0.
We simulated these differences assuming that the independence model was true, and under this condition, we expect the difference to be near zero with some random fluctuation, where *near* is pretty generous in this case since the sample sizes are so small in this study.

::: {.workedexample}
How often would you observe a difference of at least 64.3% (0.643) according to Figure \@ref(fig:malaria-rand-dot-plot)?
Often, sometimes, rarely, or never?

------------------------------------------------------------------------

It appears that a difference of at least 64.3% due to chance alone would only happen about 2% of the time according to Figure \@ref(fig:malaria-rand-dot-plot).
Such a low probability indicates a rare event.
:::

The difference of 64.3% being a rare event suggests two possible interpretations of the results of the study:

-   $H_0$: **Independence model.** The vaccine has no effect on infection rate, and we just happened to observe a difference that would only occur on a rare occasion.

-   $H_A$: **Alternative model.** The vaccine has an effect on infection rate, and the difference we observed was actually due to the vaccine being effective at combatting malaria, which explains the large difference of 64.3%.

Based on the simulations, we have two options.
(1) We conclude that the study results do not provide strong evidence against the independence model.
That is, we do not have sufficiently strong evidence to conclude the vaccine had an effect in this clinical setting.
(2) We conclude the evidence is sufficiently strong to reject $H_0$ and assert that the vaccine was useful.
When we conduct formal studies, usually we reject the notion that we just happened to observe a rare event.[\^This reasoning does not generally extend to anecdotal observations.
Each of us observes incredibly rare events every day, events we could not possibly hope to predict.
However, in the non-rigorous setting of anecdotal evidence, almost anything may appear to be a rare event, so the idea of looking for rare events in day-to-day activities is treacherous.
For example, we might look at the lottery: there was only a 1 in 292 million chance that the Powerball numbers for the largest jackpot in history (January 13th, 2016) would be (04, 08, 19, 27, 34) with a Powerball of (10), but nonetheless those numbers came up!
However, no matter what numbers had turned up, they would have had the same incredibly rare odds.
That is, *any set of numbers we could have observed would ultimately be incredibly rare*.
This type of situation is typical of our daily lives: each possible event in itself seems incredibly rare, but if we consider every alternative, those outcomes are also incredibly rare.
We should be cautious not to misinterpret such anecdotal evidence.] So in the vaccine case, we reject the independence model in favor of the alternative.
That is, we are concluding the data provide strong evidence that the vaccine provides some protection against malaria in this clinical setting.

One field of statistics, statistical inference, is built on evaluating whether such differences are due to chance.
In statistical inference, data scientists evaluate which model is most reasonable given the data.
Errors do occur, just like rare events, and we might choose the wrong model.
While we do not always choose correctly, statistical inference gives us tools to control and evaluate how often decision errors occur.
Before diving in to the formal introduction to statistical inference in Chapter \@ref(intro-stat-inference), we spend the next two chapters constructing linear and generalized linear models.

### Exercises {#case-study-malaria-vaccine-exercises}

::: {.sectionexercise}
```{r intro, child="02/exercises/02-04-case-study.Rmd"}
```
:::

## Chapter review {#chp2-review}

### Terms

We introduced the following terms in the chapter.
If you're not sure what some of these terms mean, we recommend you go back in the text and review their definitions.
We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate.
However you should be able to easily spot them as **bolded text**.

```{r}
make_terms_table(terms_chp_2)
```

### Chapter exercises {#chp2-review-exercises}

::: {.sectionexercise}
```{r intro, child="02/exercises/02-05-chapter-review.Rmd"}
```
:::

### Interactive R tutorials

Navigate the concepts you've learned in this chapter in R using the following self-paced tutorials.
All you need is your browser to get started!

::: {.alltutorials}
[Tutorial 2: Summarizing and visualizing data](https://openintrostat.github.io/ims-tutorials/02-summarizing-and-visualizing-data/)
:::

::: {.singletutorial}
[Tutorial 2 - Lesson 1: Visualizing categorical data](https://openintro.shinyapps.io/ims-02-summarizing-and-visualizing-data-01/)
:::

::: {.singletutorial}
[Tutorial 2 - Lesson 2: Visualizing numerical data](https://openintro.shinyapps.io/ims-02-summarizing-and-visualizing-data-02/)
:::

::: {.singletutorial}
[Tutorial 2 - Lesson 3: Summarizing with statistics](https://openintro.shinyapps.io/ims-02-summarizing-and-visualizing-data-03/)
:::

::: {.singletutorial}
[Tutorial 2 - Lesson 4: Case study](https://openintro.shinyapps.io/ims-02-summarizing-and-visualizing-data-04/)
:::

You can also access the full list of tutorials supporting this book [here](https://openintrostat.github.io/ims-tutorials/).

### R labs

Further apply the concepts you've learned in this chapter in R with computational labs that walk you through a data analysis case study.

::: {.singlelab}
[Intro to data - Flight delays](http://openintrostat.github.io/oilabs-tidy/02_intro_to_data/intro_to_data.html)
:::

::: {.alllabs}
[Full list of labs supporting OpenIntro::Introduction to Modern Statistics](http://openintrostat.github.io/oilabs-tidy/)
:::

# Multivariable and logistic models {#multi-logistic-models}

::: {.chapterintro}
The principles of simple linear regression lay the foundation for more sophisticated regression models used in a wide range of challenging settings.
In this chapter, we explore multiple regression, which introduces the possibility of more than one predictor in a linear model, and logistic regression, a technique for predicting categorical outcomes with two levels.
:::

## Regression with multiple predictors {#regression-multiple-predictors}

Multiple regression extends simple two-variable regression to the case that still has one response but many predictors (denoted $x_1$, $x_2$, $x_3$, ...).
The method is motivated by scenarios where many variables may be simultaneously connected to an output.

We will consider data about loans from the peer-to-peer lender, Lending Club, which is a data set we first encountered in Chapters \@ref(getting-started-with-data).
The loan data includes terms of the loan as well as information about the borrower.
The outcome variable we would like to better understand is the interest rate assigned to the loan.
For instance, all other characteristics held constant, does it matter how much debt someone already has?
Does it matter if their income has been verified?
Multiple regression will help us answer these and other questions.

The data set includes results from 10,000 loans, and we'll be looking at a subset of the available variables, some of which will be new from those we saw in earlier chapters.
The first six observations in the data set are shown in Table \@ref(tab:loans-data-matrix), and descriptions for each variable are shown in Table \@ref(tab:loans-variables).
Notice that the past bankruptcy variable (`bankruptcy`) is an indicator variable, where it takes the value 1 if the borrower had a past bankruptcy in their record and 0 if not.
Using an indicator variable in place of a category name allows for these variables to be directly used in regression.
Two of the other variables are categorical (`verified_income` and `issue_month`), each of which can take one of a few different non-numerical values; we'll discuss how these are handled in the model in Section \@ref(ind-and-cat-predictors).

::: {.data}
The data can be found in the [openintro](http://openintrostat.github.io/openintro) package: [`loans_full_schema`](http://openintrostat.github.io/openintro/reference/loans_full_schema.html).
Based on the data in this dataset we have created two new variables: `credit_util` which is calculated as the total credit utilized divided by the total credit limit and `bankruptcy` which turns the number of bankruptcies to an indicator variable (0 for no bankruptcies and 1 for at least 1 bankruptcies).
We will refer to this modified dataset as `loans`.
:::

```{r loans-data-matrix}
loans <- loans_full_schema %>%
  mutate(
    credit_util = total_credit_utilized / total_credit_limit,
    bankruptcy  = as.factor(if_else(public_record_bankrupt == 0, 0, 1)),
    verified_income = droplevels(verified_income)
    ) %>%
  rename(credit_checks = inquiries_last_12m) %>%
  select(interest_rate, verified_income, debt_to_income, credit_util, bankruptcy, term, credit_checks, issue_month) 

loans %>%
  slice_head(n = 6) %>%
  kable(caption = "First six rows from the `loans_full_schema` data set.") %>%
  kable_styling(full_width = FALSE, bootstrap_options = "striped", latex_options = "striped")
```

```{r loans-variables}
loans_var_def <- tribble(
  ~variable,         ~description,
  "interest_rate",   "Interest rate on the loan, in an annual percentage.",
  "verified_income", "Categorical variable describing whether the borrower's income source and amount have been verified, with levels `Verified`, `Source Verified`, and `Not Verified`.",
  "debt_to_income",  "Debt-to-income ratio, which is the percentage of total debt of the borrower divided by their total income.",
  "credit_util",     "Of all the credit available to the borrower, what fraction are they utilizing. For example, the credit utilization on a credit card would be the card's balance divided by the card's credit limit.",
  "bankruptcy",      "An indicator variable for whether the borrower has a past bankruptcy in their record. This variable takes a value of `1` if the answer is *yes* and `0` if the answer is *no*.",
  "term",            "The length of the loan, in months.",
  "issue_month",     "The month and year the loan was issued, which for these loans is always during the first quarter of 2018.", 
  "credit_checks",   "Number of credit checks in the last 12 months. For example, when filing an application for a credit card, it is common for the company receiving the application to run a credit check.",
)

loans_var_def %>%
  mutate(variable = cell_spec(variable, monospace = TRUE)) %>%
  kable(escape = FALSE, caption = "Variables and their descriptions for the `loans` data set.") %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), 
                latex_options = "striped")
```

### Indicator and categorical predictors {#ind-and-cat-predictors}

Let's start by fitting a linear regression model for interest rate with a single predictor indicating whether or not a person has a bankruptcy in their record:

$$\widehat{\texttt{interest_rate}} = 12.33 + 0.74 \times \texttt{bankruptcy}$$

Results of this model are shown in Table \@ref(tab:int-rate-bankruptcy).

```{r int-rate-bankruptcy}
m_bankruptcy <- lm(interest_rate ~ bankruptcy, data = loans)
tidy(m_bankruptcy) %>%
  mutate(p.value = "<0.0001") %>%
  kable(caption = "Summary of a linear model for predicting interest rate based on whether the borrower has a bankruptcy in their record. Degrees of freedom for this model is 9998.", digits = 4) %>%
  kable_styling(full_width = FALSE)
```

::: {.workedexample}
Interpret the coefficient for the past bankruptcy variable in the model.
Is this coefficient significantly different from 0?

------------------------------------------------------------------------

The variable takes one of two values: 1 when the borrower has a bankruptcy in their history and 0 otherwise.
A slope of 0.74 means that the model predicts a 0.74% higher interest rate for those borrowers with a bankruptcy in their record.
(See Section \@ref(categorical-predictor-two-levels) for a review of the interpretation for two-level categorical predictor variables.) Examining the regression output in Table \@ref(tab:int-rate-bankruptcy), we can see that the p-value for is very close to zero, indicating there is strong evidence the coefficient is different from zero when using this simple one-predictor model.
:::

Suppose we had fit a model using a 3-level categorical variable, such as `verified_income`.
The output from software is shown in Table \@ref(tab:int-rate-ver-income).
This regression output provides multiple rows for the variable.
Each row represents the relative difference for each level of `verified_income`.
However, we are missing one of the levels: `Not Verified`.
The missing level is called the **reference level** and it represents the default level that other levels are measured against.

```{r include=FALSE}
terms_chp_4 <- c("reference level")
```

```{r int-rate-ver-income}
m_verified_income <- lm(interest_rate ~ verified_income, data = loans)
tidy(m_verified_income) %>%
  mutate(p.value = "<0.0001") %>%
  kable(caption = "Summary of a linear model for predicting interest rate based on whether the borrower’s income source and amount has been verified. This predictor has three levels, which results in 2 rows in the regression output.") %>%
  kable_styling(full_width = FALSE)
```

::: {.workedexample}
How would we write an equation for this regression model?

------------------------------------------------------------------------

The equation for the regression model may be written as a model with two predictors:

$$
\begin{align}
\widehat{\texttt{interest_rate}} = 11.10 &+ 1.42 \times \texttt{verified_income}_{\texttt{Source Verified}} \\
&+ 3.25 \times \texttt{verified_income}_{\texttt{Verified}}
\end{align}
$$

We use the notation $\texttt{variable}_{\texttt{level}}$ to represent indicator variables for when the categorical variable takes a particular value.
For example, $\texttt{verified_income}_{\texttt{Source Verified}}$ would take a value of 1 if was for a loan, and it would take a value of 0 otherwise.
Likewise, $\texttt{verified_income}_{\texttt{Verified}}$ would take a value of 1 if took a value of `verified` and 0 if it took any other value.
:::

The notation $\texttt{variable}_{\texttt{level}}$ may feel a bit confusing.
Let's figure out how to use the equation for each level of the `verified_income` variable.

::: {.workedexample}
Using the model for predicting interest rate from income verification type, compute the average interest rate for borrowers whose income source and amount are both unverified.

------------------------------------------------------------------------

When `verified_income` takes a value of `Not Verified`, then both indicator functions in the equation for the linear model are set to 0:

$$\widehat{\texttt{interest_rate}} = 11.10 + 1.42 \times 0 + 3.25 \times 0 = 11.10$$

The average interest rate for these borrowers is 11.1%.
Because the level does not have its own coefficient and it is the reference value, the indicators for the other levels for this variable all drop out.
:::

::: {.workedexample}
Using the model for predicting interest rate from income verification type, compute the average interest rate for borrowers whose income source and amount are both unverified.

------------------------------------------------------------------------

When `verified_income` takes a value of `Source Verified`, then the corresponding variable takes a value of 1 while the other is 0:

$$\widehat{\texttt{interest_rate}} = 11.10 + 1.42 \times 1 + 3.25 \times 0 = 12.52$$

The average interest rate for these borrowers is 12.52%.
:::

::: {.guidedpractice}
Compute the average interest rate for borrowers whose income source and amount are both verified.[^multi-logistic-models-1]
:::

[^multi-logistic-models-1]: When `verified_income` takes a value of `Verified`, then the corresponding variable takes a value of 1 while the other is 0: $$11.10 + 1.42 \times 0 + 3.25 \times 1 = 14.35$$ The average interest rate for these borrowers is 14.35%.

::: {.important}
**Predictors with several categories.**

When fitting a regression model with a categorical variable that has $k$ levels where $k > 2$, software will provide a coefficient for $k - 1$ of those levels.
For the last level that does not receive a coefficient, this is the , and the coefficients listed for the other levels are all considered relative to this reference level.
:::

::: {.guidedpractice}
Interpret the coefficients in the model.[^multi-logistic-models-2]
:::

[^multi-logistic-models-2]: Each of the coefficients gives the incremental interest rate for the corresponding level relative to the `Not Verified` level, which is the reference level.
    For example, for a borrower whose income source and amount have been verified, the model predicts that they will have a 3.25% higher interest rate than a borrower who has not had their income source or amount verified.

The higher interest rate for borrowers who have verified their income source or amount is surprising.
Intuitively, we'd think that a loan would look *less* risky if the borrower's income has been verified.
However, note that the situation may be more complex, and there may be confounding variables that we didn't account for.
For example, perhaps lender require borrowers with poor credit to verify their income.
That is, verifying income in our data set might be a signal of some concerns about the borrower rather than a reassurance that the borrower will pay back the loan.
For this reason, the borrower could be deemed higher risk, resulting in a higher interest rate.
(What other confounding variables might explain this counter-intuitive relationship suggested by the model?)

::: {.guidedpractice}
How much larger of an interest rate would we expect for a borrower who has verified their income source and amount vs a borrower whose income source has only been verified?[^multi-logistic-models-3]
:::

[^multi-logistic-models-3]: Relative to the `Not Verified` category, the `Verified` category has an interest rate of 3.25% higher, while the `Source Verified` category is only 1.42% higher.
    Thus, `Verified` borrowers will tend to get an interest rate about $3.25% - 1.42% = 1.83%$ higher than `Source Verified` borrowers.

### Many predictors in a model

The world is complex, and it can be helpful to consider many factors at once in statistical modeling.
For example, we might like to use the full context of borrower to predict the interest rate they receive rather than using a single variable.
This is the strategy used in **multiple regression**.
While we remain cautious about making any causal interpretations using multiple regression on observational data, such models are a common first step in gaining insights or providing some evidence of a causal connection.

```{r include=FALSE}
terms_chp_4 <- c(terms_chp_4, "multiple regression")
```

We want to construct a model that accounts for not only for any past bankruptcy or whether the borrower had their income source or amount verified, but simultaneously accounts for all the variables in the `loans` data set: `verified_income`, `debt_to_income`, `credit_util`, `bankruptcy`, `term`, `issue_month`, and `credit_checks`.

$$\begin{align*}
\widehat{\texttt{interest_rate}} = b_0 &+ b_1 \times \texttt{verified_income}_{\texttt{Source Verified}} \\
&+ b_2 \times \texttt{verified_income}_{\texttt{Verified}} \\
&+ b_3 \times \texttt{debt_to_income} \\
&+ b_4 \times \texttt{credit_util} \\
&+ b_5 \times \texttt{bankruptcy} \\
&+ b_6 \times \texttt{term} \\
&+ b_7 \times \texttt{issue_month}_{\texttt{Jan-2018}} \\
&+ b_8 \times \texttt{issue_month}_{\texttt{Mar-2018}} \\
&+ b_9 \times \texttt{credit_checks}
\end{align*}$$

This equation represents a holistic approach for modeling all of the variables simultaneously.
Notice that there are two coefficients for `verified_income` and also two coefficients for `issue_month`, since both are 3-level categorical variables.

We calculate $b_0$, $b_1$, $b_2$, $\cdots$, $b_9$ the same way as we did in the case of a model with a single predictor -- we select values that minimize the sum of the squared residuals:

$$SSE = e_1^2 + e_2^2 + \dots + e_{10000}^2 = \sum_{i=1}^{10000} e_i^2 = \sum_{i=1}^{10000} \left(y_i - \hat{y}_i\right)^2$$

where $y_i$ and $\hat{y}_i$ represent the observed interest rates and their estimated values according to the model, respectively.
10,000 residuals are calculated, one for each observation.
Note that these values are sample statistics and in the case where the observed data is a random sample from a target population that we are interested in making inferences about, they are estimates of the population parameters $\beta_0$, $\beta_1$, $\beta_2$, $\cdots$, $\beta_9$.
We will discuss inference based on linear models in Chapter \@ref(inference-reg), for now we will focus on calculating sample statistics $b_i$.

We typically use a computer to minimize the sum of squares and compute point estimates, as shown in the sample output in Table \@ref(tab:loans-full).
Using this output, we identify $b_i$, just as we did in the one-predictor case.

```{r loans-full}
m_full <- lm(interest_rate ~ ., data = loans)
tidy(m_full) %>%
  mutate(
    p.value = as.character(round(p.value, 4)), 
    p.value = if_else(p.value == "0", "<0.0001", p.value)
    ) %>%
  kable(caption = "Output for the regression model, where interest rate is the outcome and the variables listed are the predictors. Degrees of freedom for this model is 9990.") %>%
  kable_styling(full_width = FALSE)
```

::: {.important}
**Multiple regression model.**

A multiple regression model is a linear model with many predictors.
In general, we write the model as

$$\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \cdots + b_k x_k$$

when there are $k$ predictors.
We always calculate $b_i$ using statistical software.
:::

::: {.workedexample}
Write out the regression model using the point estimates from Table \@ref(tab:loans-full).
How many predictors are there in this model?

------------------------------------------------------------------------

The fitted model for the interest rate is given by:

$$
\begin{align}
\widehat{\texttt{interest_rate}} = 1.925 &+ 0.975 \times \texttt{verified_income}_{\texttt{Source Verified}} \\
&+ 2.537 \times \texttt{verified_income}_{\texttt{Verified}} \\
&+ 0.021 \times \texttt{debt_to_income} \\
&+ 4.896 \times \texttt{credit_util} \\
&+ 0.386 \times \texttt{bankruptcy} \\
&+ 0.154 \times \texttt{term} \\
&+ 0.028 \times \texttt{issue_month}_{\texttt{Jan-2018}} \\
&- 0.040 \times \texttt{issue_month}_{\texttt{Mar-2018}} \\
&+ 0.228 \times \texttt{credit_checks}
\end{align}
$$

If we count up the number of predictor coefficients, we get the *effective* number of predictors in the model: $k = 9$.
Notice that the categorical predictor counts as two, once for the two levels shown in the model.
In general, a categorical predictor with $p$ different levels will be represented by $p - 1$ terms in a multiple regression model.
:::

::: {.guidedpractice}
What does $b_4$, the coefficient of variable `credit_util`, represent?
:::

::: {.guidedpractice}
Compute the residual of the first observation in Table \@ref(tab:loans-data-matrix) on page using the full model.[^multi-logistic-models-4]
:::

[^multi-logistic-models-4]: To compute the residual, we first need the predicted value, which we compute by plugging values into the equation from earlier.
    For example, $\texttt{verified_income}_{\texttt{Source Verified}}$ takes a value of 0, $\texttt{verified_income}_{\texttt{Verified}}$ takes a value of 1 (since the borrower's income source and amount were verified), was 18.01, and so on.
    This leads to a prediction of $\widehat{\texttt{interest_rate}}_1 = 18.09$.
    The observed interest rate was 14.07%, which leads to a residual of $e_1 = 14.07 - 18.09 = -4.02$.

::: {.workedexample}
We calculated a slope coefficient of 0.74 for `bankruptcy` in Section \@ref(ind-and-cat-predictors) while the coefficient is 0.386 here.
Why is there a difference between the coefficient values between the models with single and multiple predictors?

------------------------------------------------------------------------

If we examined the data carefully, we would see that some predictors are correlated.
For instance, when we modeled the relationship of the outcome `interest_rate` and predictor `bankruptcy` using simple linear regression, we were unable to control for other variables like whether the borrower had her income verified, the borrower's debt-to-income ratio, and other variables.
That original model was constructed in a vacuum and did not consider the full context.
When we include all of the variables, underlying and unintentional bias that was missed by these other variables is reduced or eliminated.
Of course, bias can still exist from other confounding variables.
:::

The previous example describes a common issue in multiple regression: correlation among predictor variables.
We say the two predictor variables are (pronounced as *co-linear*) when they are correlated, and this **collinearity** complicates model estimation.
While it is impossible to prevent collinearity from arising in observational data, experiments are usually designed to prevent predictors from being collinear.

```{r include=FALSE}
terms_chp_4 <- c(terms_chp_4, "collinearity")
```

::: {.guidedpractice}
The estimated value of the intercept is 1.925, and one might be tempted to make some interpretation of this coefficient, such as, it is the model's predicted price when each of the variables take value zero: income source is not verified, the borrower has no debt (debt-to-income and credit utilization are zero), and so on.
Is this reasonable?
Is there any value gained by making this interpretation?[^multi-logistic-models-5]
:::

[^multi-logistic-models-5]: Many of the variables do take a value 0 for at least one data point, and for those variables, it is reasonable.
    However, one variable never takes a value of zero: \texttt{term}, which describes the length of the loan, in months.
    If \texttt{term} is set to zero, then the loan must be paid back immediately; the borrower must give the money back as soon as she receives it, which means it is not a real loan.
    Ultimately, the interpretation of the intercept in this setting is not insightful.

### Adjusted R-squared

We first used $R^2$ in Section \@ref(r-squared) to determine the amount of variability in the response that was explained by the model: $$
R^2 = 1 - \frac{\text{variability in residuals}}{\text{variability in the outcome}}
    = 1 - \frac{Var(e_i)}{Var(y_i)}
$$where $e_i$ represents the residuals of the model and $y_i$ the outcomes.
This equation remains valid in the multiple regression framework, but a small enhancement can make it even more informative when comparing models.

::: {.guidedpractice}
The variance of the residuals for the model given in the earlier Guided Practice is 18.53, and the variance of the total price in all the auctions is 25.01.
Calculate $R^2$ for this model.[^multi-logistic-models-6]
:::

[^multi-logistic-models-6]: $R^2 = 1 - \frac{18.53}{25.01} = 0.2591$.

This strategy for estimating $R^2$ is acceptable when there is just a single variable.
However, it becomes less helpful when there are many variables.
The regular $R^2$ is a biased estimate of the amount of variability explained by the model when applied to a new sample of data.
To get a better estimate, we use the adjusted $R^2$.

::: {.important}
**Adjusted R-squared as a tool for model assessment**

The **adjusted R-squared** is computed as $$\begin{aligned}
  R_{adj}^{2}
    = 1 - \frac{s_{\text{residuals}}^2 / (n-k-1)}
        {s_{\text{outcome}}^2 / (n-1)}
    = 1 - \frac{s_{\text{residuals}}^2}{s_{\text{outcome}}^2}
        \times \frac{n-1}{n-k-1}
\end{aligned}$$

where $n$ is the number of cases used to fit the model and $k$ is the number of predictor variables in the model.
Remember that a categorical predictor with $p$ levels will contribute $p - 1$ to the number of variables in the model.
:::

```{r include=FALSE}
terms_chp_4 <- c(terms_chp_4, "adjusted R-squared")
```

Because $k$ is never negative, the adjusted $R^2$ will be smaller -- often times just a little smaller -- than the unadjusted $R^2$.
The reasoning behind the adjusted $R^2$ lies in the associated with each variance, which is equal to $n - k - 1$ for the multiple regression context.
If we were to make predictions for *new data* using our current model, we would find that the unadjusted $R^2$ would tend to be slightly overly optimistic, while the adjusted $R^2$ formula helps correct this bias.

::: {.guidedpractice}
There were $n=10000$ auctions in the data set and $k=9$ predictor variables in the model.
Use $n$, $k$, and the variances from the earlier Guided Practice to calculate $R_{adj}^2$ for the interest rate model.[^multi-logistic-models-7]
:::

[^multi-logistic-models-7]: $R_{adj}^2 = 1 - \frac{18.53}{25.01}\times \frac{10000-1}{1000-9-1} = 0.2584$.
    While the difference is very small, it will be important when we fine tune the model in the next section.

::: {.guidedpractice}
Suppose you added another predictor to the model, but the variance of the errors $Var(e_i)$ didn't go down.
What would happen to the $R^2$?
What would happen to the adjusted $R^2$?[^multi-logistic-models-8]
:::

[^multi-logistic-models-8]: The unadjusted $R^2$ would stay the same and the adjusted $R^2$ would go down.

Adjusted $R^2$ could have been used in Chapter \@ref(intro-linear-models).
However, when there is only $k = 1$ predictors, adjusted $R^2$ is very close to regular $R^2$, so this nuance isn't typically important when the model has only one predictor.

### Exercises {#multiple-regression-exercises}

::: {.sectionexercise}
```{r intro, child="04-exercises/04-01-regression-multiple-predictors.Rmd"}
```
:::

## Model selection {#model-selection}

The best model is not always the most complicated.
Sometimes including variables that are not evidently important can actually reduce the accuracy of predictions.
In this section, we discuss model selection strategies, which will help us eliminate variables from the model that are found to be less important.
It's common (and hip, at least in the statistical world) to refer to models that have undergone such variable pruning as **parsimonious**.

```{r include=FALSE}
terms_chp_4 <- c(terms_chp_4, "parsimonious")
```

In practice, the model that includes all available predictors is often referred to as the **full model**.
The full model may not be the best model, and if it isn't, we want to identify a smaller model that is preferable.

```{r include=FALSE}
terms_chp_4 <- c(terms_chp_4, "full model")
```

### Stepwise selection

Two common strategies for adding or removing variables in a multiple regression model are called backward elimination and forward selection.
These techniques are often referred to as **stepwise selection** strategies, because they add or delete one variable at a time as they "step" through the candidate predictors.

```{r include=FALSE}
terms_chp_4 <- c(terms_chp_4, "stepwise selection")
```

**Backward elimination** starts with the full model (the model that includes all potential predictor variables. Variables are eliminated one-at-a-time from the model until we cannot improve the model any further.

**Forward selection** is the reverse of the backward elimination technique.
Instead of eliminating variables one-at-a-time, we add variables one-at-a-time until we cannot find any variables that improve the model any further.

```{r include=FALSE}
terms_chp_4 <- c(terms_chp_4, "backward elimination", "forward selection")
```

An important consideration in implementing either of these stepwise selection strategies is the criterion used to decide whether to eliminate or add a variable.
One commonly used decision criterion is adjusted $R^2$.
When using adjusted $R^2$ as the decision criterion, we seek to eliminate or add variables depending on whether they lead to the largest improvement in adjusted $R^2$ and we stop when adding or elimination of another variable does not lead to further improvement in adjusted $R^2$.

Adjusted $R^2$ describes the strength of a model fit, and it is a useful tool for evaluating which predictors are adding value to the model, where *adding value* means they are (likely) improving the accuracy in predicting future outcomes.

Let's consider two models, which are shown in Table \@ref(tab:loans-full-for-model-selection) and Table \@ref(tab:loans-full-except-issue-month).
The first table summarizes the full model since it includes all predictors, while the second does not include the `issue_month` variable.

```{r loans-full-for-model-selection}

options(digits = 6) # to get more digits
m_full_r_sq_adj <- glance(m_full)$adj.r.squared %>% round(4)
options(digits = 3) # to get back to default set in _common.R
m_full_df_residual <- glance(m_full)$df.residual


tidy(m_full) %>%
  mutate(
    p.value = as.character(round(p.value, 4)), 
    p.value = if_else(p.value == "0", "<0.0001", p.value)
    ) %>%
  add_row(term = glue("Adjusted $R^2$ = {m_full_r_sq_adj}")) %>%
  add_row(term = glue("df = {m_full_df_residual}")) %>%
  kable(caption = "The fit for the full regression model, including the adjusted $R^2$.") %>%
  kable_styling(full_width = FALSE) %>%
  pack_rows("", 11, 12) %>%
  add_indent(11:12) %>%
  row_spec(11:12, italic = TRUE)
```

```{r loans-full-except-issue-month}
m_full_minus_issue_month <- lm(interest_rate ~ . - issue_month, data = loans)

options(digits = 6) # to get more digits
m_full_minus_issue_month_r_sq_adj <- glance(m_full_minus_issue_month)$adj.r.squared %>% round(4)
options(digits = 3) # to get back to default set in _common.R
m_full_minus_issue_month_df_residual <- glance(m_full_minus_issue_month)$df.residual

tidy(m_full_minus_issue_month) %>%
  mutate(
    p.value = as.character(round(p.value, 4)), 
    p.value = if_else(p.value == "0", "<0.0001", p.value)
    ) %>%
  add_row(term = glue("Adjusted $R^2$ = {m_full_minus_issue_month_r_sq_adj}")) %>%
  add_row(term = glue("df = {m_full_minus_issue_month_df_residual}")) %>%
  kable(caption = "The fit for the regression model after dropping the `issue_month` variable.") %>%
  kable_styling(full_width = FALSE) %>%
  pack_rows("", 9, 10) %>%
  add_indent(9:10) %>%
  row_spec(9:10, italic = TRUE)
```

::: {.workedexample}
Which of the two models is better?

------------------------------------------------------------------------

We compare the adjusted $R^2$ of each model to determine which to choose.
Since the second model has a higher $R^2_{adj}$ compared to the first model, we prefer the second model to the first.
:::

Will the model without `issue_month` be better than the model with `issue_month`?
We cannot know for sure, but based on the adjusted $R^2$, this is our best assessment.

::: {.workedexample}
Results corresponding to the full model for the `loans` data are shown in Table \@ref(tab:loans-full-for-model-selection).
How should we proceed under the backward elimination strategy?

------------------------------------------------------------------------

Our baseline adjusted $R^2$ from the full model is , and we need to determine whether dropping a predictor will improve the adjusted $R^2$.
To check, we fit models that each drop a different predictor, and we record the adjusted $R^2$:

-   Excluding `verified_income`: 0.22380
-   Excluding `debt_to_income`: 0.25468
-   Excluding `credit_util`: 0.19063
-   Excluding `bankruptcy`: 0.25787
-   Excluding `term`: 0.14581
-   Excluding `credit_checks`: 0.25854
-   Excluding `issue_month`: 0.24689

The model without `issue_month` has the highest adjusted $R^2$ of 0.25854, higher than the adjusted $R^2$ for the full model.
Because eliminating `issue_month` leads to a model with a higher adjusted $R^2$, we drop `issue_month` from the model.

Since we eliminated a predictor from the model in the first step, we see whether we should eliminate any additional predictors.
Our baseline adjusted $R^2$ is now $R^2_{adj} = 0.25854$.
We now fit new models, which consider eliminating each of the remaining predictors in addition to `issue_month`:

-   Excluding `issue_month` and `verified_income`: 0.22395
-   Excluding `issue_month` and `debt_to_income`: 0.25479
-   Excluding `issue_month` and `credit_util`: 0.19074
-   Excluding `issue_month` and `bankruptcy`: 0.25798
-   Excluding `issue_month` and `term`: 0.14592
-   Excluding `issue_month` and `credit_checks`: 0.24701

None of these models lead to an improvement in adjusted \$R\^2\$, so we do not eliminate any of the remaining predictors.
That is, after backward elimination, we are left with the model that keeps all predictors except issue_month, which we can summarize using the coefficients from Table \@ref(tab:loans-full-except-issue-month).

$$
\begin{align}  
\widehat{\texttt{interest_rate}} = 1.921 &+ 0.974 \times \texttt{verified_income}_\texttt{Source only} \\
&+ 2.535 \times \texttt{verified_income}_\texttt{Verified} \\
&+ 0.021 \times \texttt{debt_to_income} \\
&+ 4.896 \times \texttt{credit_util} \\
&+ 0.387 \times \texttt{bankruptcy} \\
&+ 0.154 \times \texttt{term} \\
&+ 0.228 \times \texttt{credit_check}
\end{align}
$$
:::

::: {.workedexample}
Construct a model for predicting `interest_rate` from the `loans` data using forward selection.

------------------------------------------------------------------------

We start with the model that includes no predictors.
Then we fit each of the possible models with just one predictor.
Then we examine the adjusted $R^2$ for each of these models:

-   Including `verified_income`: 0.05926
-   Including `debt_to_income`: 0.01946
-   Including `credit_util`: 0.06452
-   Including `bankruptcy`: 0.00222
-   Including `term`: 0.12855
-   Including `credit_checks`: -0.0001
-   Including `issue_month`: 0.01711

In this first step, we compare the adjusted $R^2$ against a baseline model that has no predictors.
The no-predictors model always has $R_{adj}^2 = 0$.
The model with one predictor that has the largest adjusted $R^2$ is the model with the `term` predictor, and because this adjusted $R^2$ is larger than the adjusted $R^2$ from the model with no predictors ($R_{adj}^2 = 0$), we will add this variable to our model.

We repeat the process again, this time considering 2-predictor models where one of the predictors is and with a new baseline of $R^2_{adj} = 0.12855$:

-   Including `term` and `verified_income`: 0.16851
-   Including `term` and `debt_to_income`: 0.14368
-   Including `term` and `credit_util`: 0.20046
-   Including `term` and `bankruptcy`: 0.13070
-   Including `term` and `credit_checks`: 0.12840
-   Including `term` and `issue_month`: 0.14294

Adding `credit_util` yields a model with a higher adjusted $R^2$ (0.20046) than the baseline (0.12855), so we also add `credit_util` to the model as a predictor.

Since we have again added a predictor to the model, we continue and see whether it would be beneficial to add a third predictor:

-   Including `term`, `credit_util, and verified_income`: 0.24183
-   Including `term`, `credit_util, and debt_to_income`: 0.20810
-   Including `term`, `credit_util, and bankruptcy`: 0.20169
-   Including `term`, `credit_util, and credit_checks`: 0.20031
-   Including `term`, `credit_util, and issue_month`: 0.21629

The model including `verified_income` improved adjusted $R^2$ (0.24183 to 0.20046), so we `verified_income` add to the model as a predictor as well.

We continue on in this way, next adding `debt_to_income`, then `credit_checks`, and `bankruptcy`.
At this point, we come again to the `issue_month` variable: adding this as a predictor leads to $R_{adj}^2 = 0.25843$, while keeping all the other predictors but excluding `issue_month` leads to a higher $R_{adj}^2 = 0.25854$.
This means we do not add `issue_month` to the model as a predictor.
In this example, we have arrived at the same model that we identified from backward elimination.
:::

::: {.important}
**Stepwise selection strategies**

Backward elimination begins with the model having the largest number of predictors and eliminates variables one-by-one until we are satisfied that all remaining variables are important to the model.
Forward selection starts with no variables included in the model, then it adds in variables according to their importance until no other important variables are found.

Backward elimination and forward selection sometimes arrive at different final models.
If trying both techniques and this happens, it's common to choose the model with the larger adjusted $R^2$.
:::

### Other model selection strategies

Stepwise selection using adjusted $R^2$ as the decision criteria is one of many widely accepted and commonly used model selection strategies.
Stepwise selection can also be carried out with decision criteria other than adjusted $R^2$, such as p-values, which you'll learn about in Chapter \@ref(inference-reg), or AIC (Akaike information criterion) or BIC (Bayesian information criterion), which you might learn about in more advanced courses.

Additionally, one can choose to include or exclude variables from a model based on expert opinion or due to research focus.

### Exercises {#model-selection-exercises}

::: {.sectionexercise}
```{r intro, child="04-exercises/04-02-model-selection.Rmd"}
```
:::

## Logistic regression {#logistic-regression}

In this section we introduce **logistic regression** as a tool for building models when there is a categorical response variable with two levels, e.g. yes and no.
Logistic regression is a type of **generalized linear model (GLM)** for response variables where regular multiple regression does not work very well.
In particular, the response variable in these settings often takes a form where residuals look completely different from the normal distribution.

```{r include=FALSE}
terms_chp_4 <- c(terms_chp_4, "logistic regression", "generalized linear model")
```

GLMs can be thought of as a two-stage modeling approach.
We first model the response variable using a probability distribution, such as the binomial or Poisson distribution.
Second, we model the parameter of the distribution using a collection of predictors and a special form of multiple regression.
Ultimately, the application of a GLM will feel very similar to multiple regression, even if some of the details are different.

### Resume data

We will consider experiment data from a study that sought to understand the effect of race and sex on job application callback rates.[@bertrand2003] To evaluate which factors were important, job postings were identified in Boston and Chicago for the study, and researchers created many fake resumes to send off to these jobs to see which would elicit a callback.[^multi-logistic-models-9]
The researchers enumerated important characteristics, such as years of experience and education details, and they used these characteristics to randomly generate the resumes.
Finally, they randomly assigned a name to each resume, where the name would imply the applicant's sex and race.

[^multi-logistic-models-9]: We did omit discussion of some structure in the data for the analysis presented: the experiment design included blocking, where typically four resumes were sent to each job: one for each inferred race/sex combination (as inferred based on the first name).
    We did not worry about this blocking aspect, since accounting for the blocking would *reduce* the standard error without notably changing the point estimates for the `race` and `sex` variables versus the analysis performed in the section.
    That is, the most interesting conclusions in the study are unaffected even when completing a more sophisticated analysis.

::: {.data}
The data from this study can be found in the [openintro](http://openintrostat.github.io/openintro) package: [`resume`](http://openintrostat.github.io/openintro/reference/resume.html).
:::

```{r resume-data-prep}
resume <- resume %>%
  rename(sex = gender) %>%
  mutate(
    sex = if_else(sex == "m", "man", "woman"),
    sex = fct_relevel(sex, "woman", "man"),
    received_callback = as.factor(received_callback),
    college_degree = as.factor(college_degree),
    honors = as.factor(honors),
    military = as.factor(military),
    has_email_address = as.factor(has_email_address)
    ) %>%
  select(received_callback, job_city, college_degree, years_experience,
         honors, military, has_email_address, race, sex)
```

The first names that were used and randomly assigned in this experiment were selected so that they would predominantly be recognized as belonging to Black or White individuals; other races were not considered in this study.
While no name would definitively be inferred as pertaining to a Black individual or to a White individual, the researchers conducted a survey to check for racial association of the names; names that did not pass this survey check were excluded from usage in the experiment.
You can find the full set of names that did pass the survey test and were ultimately used in the study in Table \@ref(tab:resume-names).
For example, Lakisha was a name that their survey indicated would be interpreted as a Black woman, while Greg was a name that would generally be interpreted to be associated with a White male.

```{r resume-names}
resume_names <- tribble(
  ~first_name, ~race, ~sex,
  "Aisha", "black", "female",
  "Allison", "white", "female",
  "Anne", "white", "female",
  "Brad", "white", "male",
  "Brendan", "white", "male",
  "Brett", "white", "male",
  "Carrie", "white", "female",
  "Darnell", "black", "male",
  "Ebony", "black", "female",
  "Emily", "white", "female",
  "Geoffrey", "white", "male",
  "Greg", "white", "male",
  "Hakim", "black", "male",
  "Jamal", "black", "male",
  "Jay", "white", "male",
  "Jermaine", "black", "male",
  "Jill", "white", "female",
  "Kareem", "black", "male",
  "Keisha", "black", "female",
  "Kenya", "black", "female",
  "Kristen", "white", "female",
  "Lakisha", "black", "female",
  "Latonya", "black", "female",
  "Latoya", "black", "female",
  "Laurie", "white", "female",
  "Leroy", "black", "male",
  "Matthew", "white", "male",
  "Meredith", "white", "female",
  "Neil", "white", "male",
  "Rasheed", "black", "male",
  "Sarah", "white", "female",
  "Tamika", "black", "female",
  "Tanisha", "black", "female",
  "Todd", "white", "male",
  "Tremayne", "black", "male",
  "Tyrone", "black", "male"
)
kable(resume_names, 
      caption = "List of all 36 unique names along with the commonly inferred race and sex associated with these names.")
```

The response variable of interest is whether or not there was a callback from the employer for the applicant, and there were 8 attributes that were randomly assigned that we'll consider, with special interest in the race and sex variables.
Race and sex are in protected classes the United States, meaning they are not legally permitted factors for hiring or employment decisions.
The full set of attributes considered is provided in Table \@ref(tab:resume-variables).

```{r resume-variables}
resume_variables <- tribble(
  ~variable,           ~description,
  "received_callback", "Specifies whether the employer called the applicant following submission of the application for the job.",
  "job_city",          "City where the job was located: Boston or Chicago.",
  "college_degree",    "An indicator for whether the resume listed a college degree.",
  "years_experience",  "Number of years of experience listed on the resume.",
  "honors",            "Indicator for the resume listing some sort of honors, e.g. employee of the month.",
  "military",          "Indicator for if the resume listed any military experience.",
  "has_email_address", "Indicator for if the resume listed an email address for the applicant.",
  "race",              "Race of the applicant, implied by their first name listed on the resume.",
  "sex",               "Sex of the applicant (limited to only and in this study), implied by the first name listed on the resume."
)

resume_variables %>%
  mutate(variable = cell_spec(variable, monospace = TRUE)) %>%
  kable(escape = FALSE, caption = "Descriptions for the `received_callback` variable along with 8 other variables in the `resume` data set. Many of the variables are indicator variables, meaning they take the value 1 if the specified characteristic is present and 0 otherwise.") %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), 
                latex_options = "striped")
```

All of the attributes listed on each resume were randomly assigned.
This means that no attributes that might be favourable or detrimental to employment would favour one demographic over another on these resumes.
Importantly, due to the experimental nature of this study, we can infer causation between these variables and the callback rate, if the variable is statistically significant.
Our analysis will allow us to compare the practical importance of each of the variables relative to each other.

### Modelling the probability of an event {#modelingTheProbabilityOfAnEvent}

Logistic regression is a generalized linear model where the outcome is a two-level categorical variable.
The outcome, $Y_i$, takes the value 1 (in our application, this represents a callback for the resume) with probability $p_i$ and the value 0 with probability $1 - p_i$.
Because each observation has a slightly different context, e.g. different education level or a different number of years of experience, the probability $p_i$ will differ for each observation.
Ultimately, it is this probability that we model in relation to the predictor variables: we will examine which resume characteristics correspond to higher or lower callback rates.

::: {.important}
**Notation for a logistic regression model.**

The outcome variable for a GLM is denoted by $Y_i$, where the index $i$ is used to represent observation $i$.
In the resume application, $Y_i$ will be used to represent whether resume $i$ received a callback ($Y_i=1$) or not ($Y_i=0$).
:::

The predictor variables are represented as follows: $x_{1,i}$ is the value of variable 1 for observation $i$, $x_{2,i}$ is the value of variable 2 for observation $i$, and so on.

$$
transformation(p_i) = b_0 + b_1 x_{1,i} + b_2 x_{2,i} + \cdots + b_k x_{k,i}
$$

We want to choose a transformation in the equation that makes practical and mathematical sense.
For example, we want a transformation that makes the range of possibilities on the left hand side of the equation equal to the range of possibilities for the right hand side; if there was no transformation for this equation, the left hand side could only take values between 0 and 1, but the right hand side could take values outside of this range.

A common transformation for $p_i$ is the logit transformation, which may be written as

$$
logit(p_i) = \log_{e}\left( \frac{p_i}{1-p_i} \right)
$$

The logit transformation is shown in Figure \@ref(fig:logit-transformation).
Below, we rewrite the equation relating $Y_i$ to its predictors using the logit transformation of $p_i$:

$$
\log_{e}\left( \frac{p_i}{1-p_i} \right) = b_0 + b_1 x_{1,i} + b_2 x_{2,i} + \cdots + b_k x_{k,i}
$$

```{r logit-transformation, fig.cap = "Values of $p_i$ against values of $logit(p_i)$."}
p <- seq(0.0001, 0.9999, 0.0002)
lp <- log(p / (1 - p))

pts <- seq(0.01, 0.99, length.out = 25)
R <- c(-6, 6)
adj <- 0.07
adj1 <- 0.02

plot(lp, p,
  xlab = expression(logit(p[i])),
  ylab = "",
  xlim = c(-5.8, 6.5),
  ylim = c(-0.05, 1.1),
  type = "n"
)
lines(lp, p,
  type = "l",
  col = COL[5],
  lwd = 1.5
)
mtext(expression(p[i]), 2, 2.4)
abline(
  h = 0:1,
  lty = 2,
  col = COL[1],
  lwd = 1.5
)
this <- which.min(abs(p - 0.2))
LP <- c(seq(6, -5, -1))
P <- exp(LP) / (1 + exp(LP))
POS <- c(3, 1, 3, 1, 2, 2, 2, 2, 4, 3, 1, 3)
xOFF <- c()
Round <- c(3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3)
for (i in 1:length(LP)) {
  points(LP[i], P[i], col = COL[4], lwd = 2)
  t1 <- format(round(c(LP, 0.9), Round[i]))[i]
  t2 <- format(round(P, Round[i]))[i]
  text(LP[i], P[i],
    paste0("(", t1, ", ", t2, ")"),
    cex = 0.6,
    pos = POS[i],
    col = COL[5]
  )
}
```

In our resume example, there are 8 predictor variables, so $k = 8$.
While the precise choice of a logit function isn't intuitive, it is based on theory that underpins generalized linear models, which is beyond the scope of this book.
Fortunately, once we fit a model using software, it will start to feel like we're back in the multiple regression context, even if the interpretation of the coefficients is more complex.

::: {.workedexample}
We start by fitting a model with a single predictor: `honors`.
This variable indicates whether the applicant had any type of honors listed on their resume, such as employee of the month.
The following logistic regression model was fit using statistical software:

$$\log_e \left( \frac{p_i}{1-p_i} \right) = -2.4998 + 0.8668 \times\text{\texttt{honors}}$$

a.  If a resume is randomly selected from the study and it does not have any honors listed, what is the probability resulted in a callback?

b.  What would the probability be if the resume did list some honors?

------------------------------------------------------------------------

(a) If a randomly chosen resume from those sent out is considered, and it does not list honors, then takes value 0 and the right side of the model equation equals -2.4998.
    Solving for $p_i$: $\frac{e^{-2.4998}}{1 + e^{-2.4998}} = 0.076$.
    Just as we labelled a fitted value of $y_i$ with a "hat" in single-variable and multiple regression, we do the same for this probability: $\hat{p}_i = 0.076{}$.

(b) If the resume had listed some honors, then the right side of the model equation is $-2.4998{} + 0.8668 \times 1 = -1.6330$, which corresponds to a probability $\hat{p}_i = 0.163$.
    Notice that we could examine -2.4998 and -1.6330 in Figure \@ref(fig:logit-transformation) to estimate the probability before formally calculating the value.
:::

To convert from values on the logistic regression scale (e.g. -2.4998 and -1.6330), use the following formula,

which is the result of solving for $p_i$ in the regression model:

$$
p_i = \frac{e^{b_0 + b_1 x_{1,i}+\cdots+b_k x_{k,i}}}{1 + e^{b_0 + b_1 x_{1,i}+\cdots+b_k x_{k,i}}}
$$

As with most applied data problems, we substitute the point estimates for the parameters (the $b_i$) so that we can make use of this formula.
The probabilities can be calculated as follows.

$$
\frac{e^{-2.4998}}{1 + e^{-2.4998}}= 0.076
$$

$$
\frac{e^{-2.4998 + 0.8668}}{1 + e^{-2.4998 + 0.8668}} = 0.163
$$

While knowing whether a resume listed honors provides some signal when predicting whether or not the employer would call, we would like to account for many different variables at once to understand how each of the different resume characteristics affected the chance of a callback.

### Logistic model with many variables

We used statistical software to fit the logistic regression model with all 8 predictors described in Table \@ref(tab:resume-variables).
Like multiple regression, the result may be presented in a summary table, which is shown in Table \@ref(tab:resume-full-fit).
The structure of this table is almost identical to that of multiple regression; the only notable difference is that the p-values are calculated using the normal distribution rather than the $t$-distribution.

```{r resume-full-fit}
resume_full_fit <- logistic_reg() %>%
  set_engine("glm") %>%
  fit(received_callback ~ job_city + college_degree + years_experience + honors + military + has_email_address + race + sex, data = resume, family = "binomial")

resume_full_fit %>%
  tidy() %>%
  mutate(p.value = "<0.0001") %>%
  kable(caption = "Summary table for the full logistic regression model for the resume callback example.") %>%
  kable_styling(full_width = FALSE)
```

Just like multiple regression, we could trim some variables from the model.
Here we'll use a statistic called **Akaike information criterion (AIC)**, which is an analogue to how we used adjusted R-squared in multiple regression, and we look for models with a lower AIC through a backward elimination strategy.
After using this criteria, the variable `college_degree` is eliminated, giving the smaller model summarized in Table \@ref(tab:resume-fit), which is what we'll rely on for the remainder of this section.

```{r include=FALSE}
terms_chp_4 <- c(terms_chp_4, "Akaike information criterion")
```

```{r resume-fit}
resume_fit <- logistic_reg() %>%
  set_engine("glm") %>%
  fit(received_callback ~ job_city + years_experience + honors + military + has_email_address + race + sex, data = resume, family = "binomial")

resume_fit %>%
  tidy() %>%
  mutate(p.value = "<0.0001") %>%
  kable(caption = "Summary table for the logistic regression model for the resume callback example, where variable selection has been performed using AIC.", digits = 4) %>%
  kable_styling(full_width = FALSE)
```

::: {.workedexample}
The `race` variable had taken only two levels: `black` and `white`.
Based on the model results, was race a meaningful factor for if a prospective employer would call back?

------------------------------------------------------------------------

We see that the p-value for this coefficient is very small (very nearly zero), which implies that race played a statistically significant role in whether a candidate received a callback.
Additionally, we see that the coefficient shown corresponds to the level of `white`, and it is positive.
This positive coefficient reflects a positive gain in callback rate for resumes where the candidate's first name implied they were White.
The data provide very strong evidence of racism by prospective employers that favours resumes where the first name is typically interpreted to be White.
:::

The coefficient of $\texttt{race}_{\texttt{white}}$ in the full model in Table \@ref(tab:resume-full-fit), is nearly identical to the model shown in Table \@ref(tab:resume-fit).
The predictors in this experiment were thoughtfully laid out so that the coefficient estimates would typically not be much influenced by which other predictors were in the model, which aligned with the motivation of the study to tease out which effects were important to getting a callback.
In most observational data, it's common for point estimates to change a little, and sometimes a lot, depending on which other variables are included in the model.

::: {.workedexample}
Use the model summarized in Table \@ref(tab:resume-fit) to estimate the probability of receiving a callback for a job in Chicago where the candidate lists 14 years experience, no honors, no military experience, includes an email address, and has a first name that implies they are a White male.

------------------------------------------------------------------------

We can start by writing out the equation using the coefficients from the model, then we can add in the corresponding values of each variable for this individual:

$$
\begin{aligned}
&log_e \left(\frac{p}{1 - p}\right) \\
&\quad= - 2.7162 - 0.4364 \times \texttt{job_city}_{\texttt{Chicago}}        + 0.0206 \times \texttt{years_experience} \\
&\quad\qquad + 0.7634 \times \texttt{honors} - 0.3443 \times \texttt{military} + 0.2221 \times \texttt{email} \\
&\quad\qquad + 0.4429 \times \texttt{race}_{\texttt{white}} - 0.1959 \times \texttt{sex}_{\texttt{man}} \\
&\quad= - 2.7162 - 0.4364 \times 1 + 0.0206 \times 14 \\
&\quad\qquad + 0.7634 \times 0 - 0.3443 \times 0 + 0.2221 \times 1 \\
&\quad\qquad + 0.4429 \times 1 - 0.1959 \times 1 \\
&\quad= - 2.3955  
\end{aligned}
$$

We can now back-solve for $p$: the chance such an individual will receive a callback is about 8.35%.
:::

::: {.workedexample}
Compute the probability of a callback for an individual with a name commonly inferred to be from a Black male but who otherwise has the same characteristics as the one described in the previous example.

------------------------------------------------------------------------

We can complete the same steps for an individual with the same characteristics who is Black, where the only difference in the calculation is that the indicator variable $\texttt{race}_{\texttt{white}}$ will take a value of 0.
Doing so yields a probability of 0.0553.
Let's compare the results with those of the previous example..

In practical terms, an individual perceived as White based on their first name would need to apply to $\frac{1}{0.0835} \approx 12$ jobs on average to receive a callback, while an individual perceived as Black based on their first name would need to apply to $\frac{1}{0.0553} \approx 18$ jobs on average to receive a callback.
That is, applicants who are perceived as Black need to apply to 50% more employers to receive a callback than someone who is perceived as White based on their first name for jobs like those in the study.
:::

What we've quantified in this section is alarming and disturbing.
However, one aspect that makes this racism so difficult to address is that the experiment, as well-designed as it is, cannot send us much signal about which employers are discriminating.
It is only possible to say that discrimination is happening, even if we cannot say which particular callbacks --- or non-callbacks --- represent discrimination.
Finding strong evidence of racism for individual cases is a persistent challenge in enforcing anti-discrimination laws.

### Model diagnostics

::: {.important}
**Logistic regression conditions.**

There are two key conditions for fitting a logistic regression model:

1.  Each outcome $Y_i$ is independent of the other outcomes.
2.  Each predictor $x_i$ is linearly related to logit$(p_i)$ if all other predictors are held constant.
:::

The first logistic regression model condition --- independence of the outcomes --- is reasonable for the experiment since characteristics of resumes were randomly assigned to the resumes that were sent out.

The second condition of the logistic regression model is not easily checked without a fairly sizable amount of data.
Luckily, we have 4870 resume submissions in the data set!
Let's first visualize these data by plotting the true classification of the resumes against the model's fitted probabilities, as shown in Figure \@ref(fig:resume-predict).

```{r resume-predict, fig.cap = "The predicted probability that each of the 4870 resumes results in a callback. Noise (small, random vertical shifts) have been added to each point so points with nearly identical values aren’t plotted exactly on top of one another."}
resume_pred <- predict(resume_fit, new_data = resume, type = "prob") %>%
  bind_cols(resume %>% select(received_callback))

ggplot(resume_pred, aes(x = .pred_1, y = received_callback)) +
  geom_jitter(color = COL["blue", "full"], alpha = 0.15) +
  coord_cartesian(xlim = c(0, 1))
```

<!--# Omitting bucket diagram for diagnostics since it uses 95% CI which we haven't discussed yet. Should it go in the final chapter? -->

### Groups of different sizes

Any form of discrimination is concerning, and this is why we decided it was so important to discuss this topic using data.
The resume study also only examined discrimination in a single aspect: whether a prospective employer would call a candidate who submitted their resume.
There was a 50% higher barrier for resumes simply when the candidate had a first name that was perceived to be from a Black individual.
It's unlikely that discrimination would stop there.

::: {.workedexample}
Let's consider a sex-imbalanced company that consists of 20% women and 80%[^multi-logistic-models-10] men, and we'll suppose that the company is very large, consisting of perhaps 20,000 employees.
Suppose when someone goes up for promotion at this company, 5 of their colleagues are randomly chosen to provide feedback on their work.

Now let's imagine that 10% of the people in the company are prejudiced against the other sex.
That is, 10% of men are prejudiced against women, and similarly, 10% of women are prejudiced against men.

Who is discriminated against more at the company, men or women?

------------------------------------------------------------------------

Let's suppose we took 100 men who have gone up for promotion in the past few years.
For these men, $5 \times 100 = 500$ random colleagues will be tapped for their feedback, of which about 20% will be women (100 women).
Of these 100 women, 10 are expected to be biased against the man they are reviewing.
Then, of the 500 colleagues reviewing them, men will experience discrimination by about 2% of their colleagues when they go up for promotion.

Let's do a similar calculation for 100 women who have gone up for promotion in the last few years.
They will also have 500 random colleagues providing feedback, of which about 400 (80%) will be men.
Of these 400 men, about 40 (10%) hold a bias against women.
Of the 500 colleagues providing feedback on the promotion packet for these women, 8% of the colleagues hold a bias against the women.
:::

[^multi-logistic-models-10]: A more thoughtful example would include non-binary individuals.

This example highlights something profound: even in a hypothetical setting where each demographic has the same degree of prejudice against the other demographic, the smaller group experiences the negative effects more frequently.
Additionally, if we would complete a handful of examples like the one above with different numbers, we'd learn that the greater the imbalance in the population groups, the more the smaller group is disproportionately impacted.[^multi-logistic-models-11]

[^multi-logistic-models-11]: If a proportion $p$ of a company are women and the rest of the company consists of men, then under the hypothetical situation the ratio of rates of discrimination against women vs men would be given by $\frac{1 - p}{p}$; this ratio is always greater than 1 when $p < 0.5$.

Of course, there are other considerable real-world omissions from the hypothetical example.
For example, studies have found instances where people from an oppressed group also discriminate against others within their own oppressed group.
As another example, there are also instances where a majority group can be oppressed, with apartheid in South Africa being one such historic example.
Ultimately, discrimination is complex, and there are many factors at play beyond the mathematics property we observed in the previous example.

We close this book on this serious topic, and we hope it inspires you to think about the power of reasoning with data.
Whether it is with a formal statistical model or by using critical thinking skills to structure a problem, we hope the ideas you have learned will help you do more and do better in life.

### Exercises {#logistic-regression-exercises}

::: {.sectionexercise}
```{r intro, child="04-exercises/04-05-logistic-regression.Rmd"}
```
:::

## Chapter review {#chp4-review}

### Terms

We introduced the following terms in the chapter.
If you're not sure what some of these terms mean, we recommend you go back in the text and review their definitions.
We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate.
However you should be able to easily spot them as **bolded text**.

```{r}
make_terms_table(terms_chp_4)
```

### Chapter exercises {#chp4-review-exercises}

::: {.sectionexercise}
```{r intro, child="04-exercises/04-06-chapter-review.Rmd"}
```
:::

### Interactive R tutorials

Navigate the concepts you've learned in this chapter in R using the following self-paced tutorials.
All you need is your browser to get started!

::: {.alltutorials}
[Tutorial 4: Multiple and logistic regression](https://openintrostat.github.io/ims-tutorials/04-multivariable-and-logistic-models/)
:::

::: {.singletutorial}
[Tutorial 4 - Lesson 1: Parallel slopes](https://openintro.shinyapps.io/ims-04-multivariable-and-logistic-models-01/)
:::

::: {.singletutorial}
[Tutorial 4 - Lesson 2: Evaluating and extending parallel slopes model](https://openintro.shinyapps.io/ims-04-multivariable-and-logistic-models-02/)
:::

::: {.singletutorial}
[Tutorial 4 - Lesson 3: Multiple regression](https://openintro.shinyapps.io/ims-04-multivariable-and-logistic-models-03/)
:::

::: {.singletutorial}
[Tutorial 4 - Lesson 4: Logistic regression](https://openintro.shinyapps.io/ims-04-multivariable-and-logistic-models-04/)
:::

::: {.singletutorial}
[Tutorial 4 - Lesson 5: Case study - Italian restaurants in NYC](https://openintro.shinyapps.io/ims-04-multivariable-and-logistic-models-05/)
:::

You can also access the full list of tutorials supporting this book [here](https://openintrostat.github.io/ims-tutorials/).

### R labs

Further apply the concepts you've learned in this chapter in R with computational labs that walk you through a data analysis case study.

::: {.singlelab}
[Multiple linear regression - Grading the professor](http://openintrostat.github.io/oilabs-tidy/09_multiple_regression/multiple_regression.html)
:::

::: {.alllabs}
[Full list of labs supporting OpenIntro::Introduction to Modern Statistics](http://openintrostat.github.io/oilabs-tidy/)
:::
